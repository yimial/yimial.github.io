{"meta":{"title":"Hexo","subtitle":"","description":"","author":"执杭","url":"http://yimial.github.io","root":"/"},"pages":[{"title":"about","date":"2020-07-07T14:38:22.000Z","updated":"2020-07-07T14:39:22.622Z","comments":true,"path":"about/index.html","permalink":"http://yimial.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-07-07T14:39:07.000Z","updated":"2020-07-07T14:39:43.322Z","comments":true,"path":"categories/index.html","permalink":"http://yimial.github.io/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2020-07-07T14:38:56.000Z","updated":"2020-07-07T14:39:50.243Z","comments":true,"path":"friends/index.html","permalink":"http://yimial.github.io/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-07-07T14:38:44.000Z","updated":"2020-07-07T14:39:57.372Z","comments":true,"path":"tags/index.html","permalink":"http://yimial.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"A Survey on Knowledge Graphs: Representation, Acquisition and Applications的知识获取章节概览及相关论文列表","slug":"2020 KG survey KA","date":"2020-07-09T13:43:39.000Z","updated":"2020-07-09T18:05:58.758Z","comments":true,"path":"2020/07/09/2020 KG survey KA/","link":"","permalink":"http://yimial.github.io/2020/07/09/2020%20KG%20survey%20KA/","excerpt":"","text":"基本上是对该章节的翻译和整理，由于英文水平有限，找不到达意的翻译我就用英文单词代替，之后会对其中的几篇论文进行阅读，精读后再返回来做修改 4. 知识获取知识获取的目的是从非结构化文本中构造知识图谱，完成现有的知识图谱，发现和识别实体和关系。构造良好和大规模的知识知识图谱对许多下游应用程序非常有用，并赋予了知识感知模型常识推理的能力，从而为人工智能铺平了道路。知识获取的主要任务包括关系抽取、KGC(knowledge graph completion)以及实体识别、实体对齐等面向实体的获取任务。大多数方法分别进行KGC和关系抽取。然而，这两个任务也可以集成到一个统一的框架中。 • X. Han, Z. Liu, and M. Sun, “Neural knowledge acquisition via mutual attention between knowledge graph and text,” in AAAI, 2018, pp. 4832–4839.：联合学习框架/相互注意力/知识图和文本的数据融合[1]还有其他与知识获取相关的任务，如三重分类、关系分类等。 本章分为三个部分KGC、实体识别、关系抽取4.1 KGCcomplete现有实体之间缺失的链接或给定实体和关系查询推断出实体集。包括链路预测、实体预测和关系预测。 初始阶段研究的内容是三元组的低维嵌入4.1.1 基于嵌入的方法：缺点：无法捕捉multi-step的关系，忽视知识图的符号化性质，缺乏可解释性，因而仍停留在个体关系层面，在复杂推理方面表现较差基于已存在的三元组学习出嵌入向量，然后用E中的所有实体替换头实体或尾实体来计算候选实体对的得分，并排名出前k个实体对 最近研究内容转向对multi-step关系路径和incorporate（包含）逻辑规则的探索4.1.2 基于规则的推理符号与嵌入的混合方法引入了基于规则的推理，克服了知识图的稀疏性，提高了嵌入的质量，方便了规则的有效注入，并归纳出了可解释规则。 4.1.3 关系路径推理通过观察知识图的图形性质，研究了路径搜索和神经路径表示学习，但它们在遍历大规模图时存在连通性不足的问题。 4.1.4 基于强化学习的路经查找4.1.5 元关系查找元关系学习的新兴方向是学习在低资源环境中快速适应不可见的关系。 4.1.6 三元组分类KGC的相关任务，评估真实的三元组的正确性 4.2 实体发现4.2.1 实体识别在文本中标记实体 人工特征 使用端到端的神经网络来学习特征 • J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional LSTM-CNNs,” Transactions of ACL, vol. 4, pp. 357–370, 2016.学习字符级和单词级特征，编码部分词汇匹配 • G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural architectures for named entity recognition,” in NAACL, 2016, pp. 260–270.通过LSTM层和CRF层的叠加，提出了多层神经结构 • C. Xia, C. Zhang, T. Yang, Y. Li, N. Du, X. Wu, W. Fan, F. Ma, and P. Yu, “Multi-grained named entity recognition,” in ACL, 2019, pp.1430–1440.针对嵌套和非重叠命名实体，集成了不同粒度的实体位置检测和基于注意的实体分类框架 4.2.2 实体分类以序列到序列的方式对实体识别进行了探索，实体类型分类讨论了有噪声的类型标签和零击类型实体类型包括粗粒度类型和细粒度类型，而后者使用树结构型类别，通常被视为多类和多标签分类。 • X. Ren, W. He, M. Qu, C. R. Voss, H. Ji, and J. Han, “Label noise reduction in entity typing by heterogeneous partial-label embedding,” in SIGKDD, 2016, pp. 1825–1834.集中于正确的类型识别，提出了一种异构图的局部标签嵌入模型，用于表示实体提及词、文本特征和实体类型及其关系。 • Y. Ma, E. Cambria, and S. Gao, “Label embedding for zero-shot fine-grained named entity typing,” in COLING, 2016, pp. 171–180 为了解决类型集的不断增长和标签的噪声，提出包含分层信息的原型驱动标签，用于zero-shot细粒度命名实体类型识别 4.2.3 实体消歧 实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。目前流行的端到端学习方法都是通过实体和提及词的表示学习来实现的 • H. Huang, L. Heck, and H. Ji, “Leveraging deep neural networks and knowledge graphs for entity disambiguation,” arXiv preprint arXiv:1504.07678, 2015.对实体语义相似度建模 • W. Fang, J. Zhang, D. Wang, Z. Chen, and M. Li, “Entity disambiguation by knowledge and text jointly embedding,” in SIGNLL, 2016, pp. 260–269.对实体和文本的联合embedding建模 • O.-E. Ganea and T. Hofmann, “Deep joint entity disambiguation with local neural attention,” in EMNLP, 2017, pp. 2619–2629.提出了一种基于局部上下文窗口的注意力神经模型，用于学习实体embedding和“用于推理模糊实体”的可分辨消息传递 • P. Le and I. Titov, “Improving entity linking by modeling latent relations between mentions,” in ACL, vol. 1, 2018, pp. 1595–1604.将实体之间的关系视为潜在变量，提出了一种在关系层和提及词层进行归一化的端到端神经网络。4.2.4 实体对齐在异构知识图中融合知识给定两个不同知识图谱的两个实体集，实体对齐的任务是找到A = {(e 1 ,e 2 ) ∈ E 1 × E 2 |e 1 ≡ e 2 }，≡表示实体间的相等关系。在实际操作中，会给定一小组对齐种子来启动对齐过程。如果新对齐的实体性能不佳，则可能会面临错误累积问题 基于embedding的对齐：计算一对实体的embedding之间的相似性。• H. Zhu, R. Xie, Z. Liu, and M. Sun, “Iterative entity alignment via joint knowledge embeddings,” in IJCAI, 2017, pp. 4258–4264.基于一个联合embedding框架，通过对齐转换、线性转换和参数共享把实体映射到一个表示空间 • Z. Sun, W. Hu, Q. Zhang, and Y. Qu, “Bootstrapping entity alignment with knowledge graph embedding.” in IJCAI, 2018,pp. 4396–4402.为了解决迭代对齐中的误差积累问题，BootEA[100]提出了一种增量训练方式的bootstrapping方法，以及一种检查新标记对齐的编辑技术。 其他研究合并了实体的其他信息以进行细化，近年来，特定语言知识的增加，必然推动了跨语言知识整合的研究。• Z. Sun, W. Hu, and C. Li, “Cross-lingual entity alignment via joint attribute-preserving embedding,” in ISWC, 2017, pp. 628–644.捕获跨语言属性之间的相关性 • M. Chen, Y. Tian, K.-W. Chang, S. Skiena, and C. Zaniolo, “Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment,” in IJCAI, 2018, pp. 3998–4004.通过联合训练嵌入多语言实体描述 • Q. Zhang, Z. Sun, W. Hu, M. Chen, L. Guo, and Y. Qu, “Multi-view knowledge graph embedding for entity alignment,” in IJCAI, 2019,pp. 5429–5435.学习实体名称、关系和属性的多个视图 • B. D. Trsedya, J. Qi, and R. Zhang, “Entity alignment between knowledge graphs using attribute embeddings,” in AAAI, vol. 33,2019, pp. 297–304.使用字符属性embedding对齐4.3 关系抽取从纯文本中提取未知关系事实，并将其添加到知识图谱中，是自动构建大规模知识图谱的关键任务 由于缺乏带标签的关系数据，远程监督在关系数据库的监督下，采用启发式匹配的方法，假设包含相同实体成分的句子可能表达相同的关系来创建训练数据。在远程监督的假设下，关系提取存在噪声，特别是在不同领域的文本语料库中。因此，弱监督关系提取对于减轻噪声标注的影响非常重要 传统方法高度依赖特征工程• M. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in ACL and IJCNLP of the AFNLP, 2009, pp. 1003–1011.Mintz等[106]采用远程监督的方法对关系进行分类，其文本特征包括词汇和句法特征、命名实体标签和连接词特征最近的一种方法是探索特征之间的内在关联4.3.1 神经关系提取• D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classifica- tion via convolutional deep neural network,” in COLING, 2014, pp. 2335–2344.使用到实体的相对位置特征的CNN网络被首先应用于关系分类 • T. H. Nguyen and R. Grishman, “Relation extraction: Perspective from convolutional neural networks,” in ACL Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 39–48.扩展到使用多尺度卷积核的多窗口CNN用于关系抽取多实例学习以一个句子包作为输入，预测实体对之间的关系• D. Zeng, K. Liu, Y. Chen, and J. Zhao, “Distant supervision for relation extraction via piecewise convolutional neural networks,”in EMNLP, 2015, pp. 1753–1762.与传统CNN[Zeng]相比，PCNN能够更有效地捕获实体对内部的结构信息。根据实体位置分段的最大池化 • X. Jiang, Q. Wang, P. Li, and B. Wang, “Relation extraction with multi-instance multi-label convolutional neural networks,” in COLING, 2016, pp. 1471–1480.MIMLCNN进一步将其扩展到多标签学习，利用跨句最大池化进行特征选择。 • H. Ye, W. Chao, Z. Luo, and Z. Li, “Jointly extracting relations with class ties via effective deep ranking,” in ACL, vol. 1, 2017, pp.1810–1820.利用class ties进行 • W. Zeng, Y. Lin, Z. Liu, and M. Sun, “Incorporating relation paths in neural relation extraction,” in EMNLP, 2017, pp. 1768–1777.利用关系路径RNNs也被引入• Y. Xu, L. Mou, G. Li, Y. Chen, H. Peng, and Z. Jin, “Classifying relations via long short term memory networks along shortest dependency paths,” in EMNLP, 2015, pp. 1785–1794.采用多通道LSTM，利用实体对之间的最短依赖路径 • M. Miwa and M. Bansal, “End-to-end relation extraction using lstms on sequences and tree structures,” in ACL, vol. 1, 2016, pp.1105–1116.Miwa等基于依赖树的序列和树结构LSTM • R. Cai, X. Zhang, and H. Wang, “Bidirectional recurrent convolutional neural network for relation classification,” in ACL, vol. 1,2016, pp. 756–765.BRCNN利用双通道双向LSTM和CNN，将捕获序列依赖关系的RNN和表示局部语义的CNN结合起来4.3.2 注意力机制各种各样的注意力机制都可以和CNN进行结合 • Y. Shen and X. Huang, “Attention-based convolutional neural network for semantic relation extraction,” in COLING, 2016, pp.2526–2536.词级别的注意力捕捉词的语义信息 • Y. Lin, S. Shen, Z. Liu, H. Luan, and M. Sun, “Neural relation extraction with selective attention over instances,” in ACL, vol. 1,2016, pp. 2124–2133.对多个实例的选择性注意力，以减轻噪声实例的影响 • G. Ji, K. Liu, S. He, and J. Zhao, “Distant supervision for relation extraction with sentence-level attention and entity descriptions,”in AAAI, 2017, pp. 3060–3066.APCNN引入了PCNN的实体描述和句子级注意 • X. Han, P. Yu, Z. Liu, M. Sun, and P. Li, “Hierarchical relation extraction with coarse-to-fine grained attention,” in EMNLP, 2018,pp. 2236–2245.HATT提出分层选择性注意力，通过连接每个层级的注意力表示来捕获关系的层次 • P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu, “Attention-based bidirectional long short-term memory networks for relation classification,” in ACL, vol. 2, 2016, pp. 207–212.Att-BLSTM没有使用基于CNN的句子编码器，而是使用BiLSTM提出了词级注意力4.3.3 图卷积网络GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码 • Y. Zhang, P. Qi, and C. D. Manning, “Graph convolution over pruned dependency trees improves relation extraction,” in EMNLP, 2018, pp. 2205–2215.基于path-centric剪枝后的句子依赖树的上下文GCN模型 • Z. Guo, Y. Zhang, and W. Lu, “Attention guided graph convolutional networks for relation extraction,” in ACL, 2019, pp. 241–251.AGGCN也在依赖树上应用GCN，但采用软加权方式的多头注意力方式进行边的选择 • N. Zhang, S. Deng, Z. Sun, G. Wang, X. Chen, W. Zhang, and H. Chen, “Long-tail relation extraction via knowledge graph embeddings and graph convolution networks,” in NAACL, 2019,pp. 3016–3025.将GCN用于知识图谱中关系的embedding，从而实现基于语句的关系抽取。4.3.4 对抗训练在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声 • Y. Wu, D. Bamman, and S. Russell, “Adversarial training for relation extraction,” in EMNLP, 2017, pp. 1778–1783. • P. Qin, X. Weiran, and W. Y. Wang, “DSGAN: Generative adversarial training for distant supervision relation extraction,” in ACL,vol. 1, 2018, pp. 496–505.DSGAN通过学习一个语句级true position的生成器和一个使生成器的true position概率最小化的识别器来去除远程监督关系抽取中的噪声4.3.5 强化学习近年来，通过用策略网络训练实例选择器的方式将强化学习集成到神经关系抽取中。基于强化学习的NRE的优点是关系抽取是 model-agnostic的。因此，它可以很容易地适应于任何神经结构，以有效地提取关系。 • P. Qin, W. Xu, and W. Y. Wang, “Robust distant supervision relation extraction via deep reinforcement learning,” in ACL, vol. 1,2018, pp. 2137–2147 提出训练基于策略的句子关系分类器的RL代理将假阳性实例重新分布到负样本中，以减轻噪声数据的影响。以F1得分为评价指标，采用基于F1得分的性能变化作为策略网络的奖励 • X. Zeng, S. He, K. Liu, and J. Zhao, “Large scaled relation extraction with reinforcement learning,” in AAAI, 2018, pp. 5658–5665.和J. Feng, M. Huang, L. Zhao, Y. Yang, and X. Zhu, “Reinforcement learning for relation classification from noisy data,” in AAAI, 2018, pp. 5779–5786.提出了其他的奖励策略， • R. Takanobu, T. Zhang, J. Liu, and M. Huang, “A hierarchical framework for relation extraction with reinforcement learning,” in AAAI, vol. 33, 2019, pp. 7072–707 提出了一个高级关系检测和低级实体提取的层次策略学习框架4.3.6 其他进展• Y. Huang and W. Y. Wang, “Deep residual learning for weakly- supervised relation extraction,” in EMNLP, 2017, pp. 1803–1807.将深度残差学习应用到噪声关系提取中，发现9层cnn的性能得到了提高。 • T. Liu, X. Zhang, W. Zhou, and W. Jia, “Neural relation extraction via inner-sentence noise reduction and transfer learning,” in EMNLP, 2018, pp. 2195–2204.提出通过实体分类中的迁移学习来初始化神经模型 • K. Lei, D. Chen, Y. Li, N. Du, M. Yang, W. Fan, and Y. Shen, “Co- operative denoising for distantly supervised relation extraction,” in COLING, 2018, pp. 426–436.通过双向知识蒸馏和自适应模拟，将文本语料库和知识图谱与外部逻辑规则集成在一起 • H. Jiang, L. Cui, Z. Xu, D. Yang, J. Chen, C. Li, J. Liu, J. Liang, C. Wang, Y. Xiao, and W. Wang, “Relation extraction using supervision from topic knowledge of relation labels,” in IJCAI, 2019, pp. 5024–5030.通过匹配句子和主题词，丰富句子表征学习 • T. Gao, X. Han, Z. Liu, and M. Sun, “Hybrid attention-based prototypical networks for noisy few-shot relation classification,” in AAAI, vol. 33, 2019, pp. 6407–6414.提出了基于混合注意力的原型网络来计算原型关系embedding，并比较其和query的embedding之间的距离。","categories":[],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yimial.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"关系抽取","slug":"关系抽取","permalink":"http://yimial.github.io/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"},{"name":"实体识别","slug":"实体识别","permalink":"http://yimial.github.io/tags/%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"}]},{"title":"Lambda 架构","slug":"Lambda","date":"2020-07-07T14:07:28.889Z","updated":"2020-07-07T14:54:08.773Z","comments":true,"path":"2020/07/07/Lambda/","link":"","permalink":"http://yimial.github.io/2020/07/07/Lambda/","excerpt":"","text":"什么是Lambda架构？Nathan Marz根据他在Backtype和Twitter从事分布式数据处理系统工作的经验，提出了Lambda架构(LA)这个术语，用于通用的、可伸缩的、容错的数据处理架构。LA旨在满足对具有容错性(包括硬件故障和人为错误)的健壮系统的需求，它能够广泛服务于各种工作负载和用例，并且满足了低延迟读取和更新的需求。该系统应该是线性可伸缩的，并且应该向外扩展（横向扩展）而不是向上扩展（纵向扩展）。 所有进入系统的数据都被发送到batch layer和speed layer进行处理。 batch layer有两个功能:(i)管理主数据集(一个不可变的、只追加的原始数据集)，(ii)预计算批处理视图。 serving layer对批处理视图进行索引，以便以低延迟、特别的方式查询它们。 speed layer补偿了serving layer更新的高延迟问题，并且只处理最近的数据。 任何传入的查询都可以通过合并来自批处理视图和实时视图的结果来回答。 组件批处理组件：处理框架 服务组件：合并/低延迟数据库 速度组件：流处理框架、基于云计算的offerings、更多资源（流处理、机器学习、Storm）","categories":[],"tags":[]},{"title":"Questioning the Lambda Architecture阅读笔记（翻译）","slug":"Questioning the Lambda Architecture","date":"2019-11-12T12:42:25.000Z","updated":"2020-07-09T14:50:22.053Z","comments":true,"path":"2019/11/12/Questioning the Lambda Architecture/","link":"","permalink":"http://yimial.github.io/2019/11/12/Questioning%20the%20Lambda%20Architecture/","excerpt":"","text":"Lambda架构有其优点，但值得探索其他替代方案。作者先介绍了一下Lambda系统，它的优点、缺点，以及替换方案 一、引言Lambda架构是在MapReduce和Storm或类似系统上构建流处理应用程序的一种方法。 其工作方式是捕获不可变的记录序列并并行地输入批处理系统和流处理系统。需要两次实现转换逻辑，一次在批处理系统中，一次在流处理系统中。在查询时将来自两个系统的结果拼接在一起，以生成完整的答案。 Lambda系统：一个记录序列同时输入两个系统，批处理系统隔一段时间统一预计算一次，实时系统及时进行增量计算，当某一段记录在批处理统一计算时经过了批处理系统，即可在实时系统中删除这些记录的结果。查询到来时，将批处理视图和实时视图直接拼接合并就是最新的数据。批处理视图存在HDFS中，实时视图存在HBase中，拼接后使用可以在这两个存储系统上查询的Impala进行查询 这其中还可以有一些变化，比如换掉Storm、Kafka和Hadoop。人们通常使用两个不同的数据库来存储输出表，一个用于优化实时处理，另一个用于优化批量更新。 Lambda架构可用于处理那些围绕复杂的异步转换构建的应用程序，这些转换需要以低延迟运行(例如，几秒钟到几小时)。比如新闻推荐系统，它需要抓取各种新闻来源，处理和标准化所有的输入，然后建立索引、排序，并将其存储以供服务。 Jay Kreps认为这种方法并不是处理实时数据的最好的方式，并列出了一些优缺点 二、Lambda的优点输入数据的不可变性：把data transformation看成从输入数据出发的一系列实例化阶段是非常好的。这是使大型MapReduce工作流易于处理的原因之一，因为它使您能够独立地调试每个阶段。Jay Kreps写了一些关于捕获和转换不可变数据流的想法。 数据reprocessing：数据reprocessing是流处理的关键挑战之一，但常常被忽略。代码总是会改变的。因此，如果您有从输入流派生输出数据的代码，每当代码更改时，您都需要重新计算输出以查看更改的效果。有很多原因会造成修改代码，不论为什么要修改代码，你都需要重新生成你的输出，我发现，许多构建实时数据处理系统的人并没有在这个问题上花太多心思，最终导致系统无法快速发展，因为没有方便的方法来处理reprocessing。Lambda体系结构强调了这个问题，值得赞扬。 Lambda架构的提出还有一些其他动机，但是我认为它们没有多大意义。一个是实时处理本质上是近似计算，比批处理更弱，误差更大。我认为不是这样。确实，现有的流处理框架集不如MapReduce成熟，但是没有理由证明流处理系统不能提供与批处理系统一样强大的语义保证。什么叫做强大的语义保证，是指向实时处理并不是一种近似计算吗？为什么有人说实时处理是近似计算？如何证明实时处理不是近似计算？ 还有另一种解释是Lambda架构通过使用不同的权衡混合不同的数据系统，以某种方式“击败了CAP定理”。长话短说，虽然流处理中确实存在延迟/可用性权衡，但这是异步处理的体系结构，因此计算的结果不会立即与传入数据保持一致。遗憾的是，CAP定理仍然未被打破。为什么异步处理的体系结构计算的结果不会立即与传入数据保持一致？我看着挺一致的啊？ 三、Lambda的缺点Lambda架构的问题在于，需要保持两个复杂的分布式系统会生成相同的计算结果，这就像听上去这么痛苦。Jay Kreps认为这个问题解决不了。在Storm和Hadoop这样的分布式框架中进行编程非常复杂。代码会被构建成相应框架的形式。实现Lambda架构的系统的操作复杂性似乎是每个人都一致同意的。 为什么不能改进流处理系统来处理其目标域中的所有问题？解决此问题的一种建议方法是使用一种语言或框架，该语言或框架对实时和批处理框架进行抽象。开发者使用这个高级框架来编写代码，然后它“向下编译”以进行流处理或MapReduce。Summingbird就是这样一个框架。这确实让事情好转了一点，但我不认为它解决了问题。 最后，即使可以避免对应用程序进行两次编码，运行和调试两个系统的操作负担也会非常大。任何一个新的抽象都只能提供这两个系统的交集所支持的特性。更糟糕的是，使用这种新的超级框架隔离了使Hadoop成为如此强大的丰富的生态系统的工具和语言(Hive、Pig、Crunch、Cascading、Oozie等)。 使跨数据库ORM真正透明是很困难的。和对那些非常相似的使用标准化的接口语言提供几乎相同的功能的系统进行抽象的问题相比，在几乎不稳定的分布式系统上构建的完全不同的编程范例是非常困难的这一段没懂，什么是透明，什么是跨数据库ORM，这一段从哪里来，要到哪里去 四、作者的工作Jay Kreps在LinkedIn做了很多尝试，混合Hadoop架构甚至是一个特定领域的API，它实现了Hadoop层和实时处理层的透明化。但是这些方法都没有令人满意。隐藏底层框架的方法被证明是最有漏洞的。它最终还是需要深入了解Hadoop知识以及实时层的知识——除此之外还增加了一个新要求，即无论何时要调试或计算性能时，您都要非常了解如何将API转换为这些底层系统。 所以Jay Kreps的建议是，如果您对延迟不敏感，可以使用批处理框架(如MapReduce)，如果您对延迟敏感，可以使用流处理框架，但不要尝试同时进行这两种处理，除非您必须同时进行。 那么，为什么Lambda架构如此令人激动呢?Jay Kreps认为原因是人们越来越需要构建复杂的、低延迟的处理系统。他们手头有两个不能完全解决的问题:一个可伸缩的高延迟批处理系统可以处理历史数据，一个低延迟流处理系统不能reprocess结果。通过管道把这两个东西粘在一起，就建立了一个有用的解决方案。 在这个意义上，即使Lambda架构是痛苦的，它依然解决了一个通常会被忽略的重要问题。但Jay Kreps不认为这是一个新的范式或大数据的未来。它只是被现有工具的限制所驱动的一个临时状态。Jay Kreps认为有更好的选择。 五、替代方案作为一个设计基础设施的人，Jay Kreps认为最突出的问题是:为什么不能改进流处理系统来处理其目标领域的所有问题?为什么需要粘在另一个系统上?为什么不能同时进行实时处理和处理代码更改时的reprocessing?流处理系统已经有了并行的概念;为什么不通过增加并行度和非常快地重放历史来处理reprocessing呢?答案是，你可以这样做，我认为这实际上是一个合理的替代架构，如果你正在构建这种类型的系统。 当我与人讨论这个问题时，他们有时告诉我，流处理不适合于处理高吞吐量的历史数据。但我认为这种直觉主要基于他们所使用的系统的局限性，这些系统要么伸缩性很差，要么无法保存历史数据。这让他们有一种感觉，流处理系统本质上是计算一些短暂流的结果，然后丢弃所有底层数据的东西。但没有理由认为只能这样。 流处理中的基本抽象是数据流DAGs。DAG与传统数据仓库(la Volcano)中的底层抽象概念完全相同，也是MapReduce后续Tez中的底层抽象概念。流处理只是这个数据流模型的概况，它向最终用户公开中间结果的检查点和连续输出。为什么说DAG和传统数据仓库底层抽象概念完全相同？这一段又是想说明什么？ 那么，我们如何直接从流处理工作中进行reprocessing呢?Jay Kreps喜欢的方法其实非常简单:1.使用Kafka或其他系统，可以保留您希望能够重新处理的、允许多用户访问的数据的完整日志。例如，如果您希望重新处理最多30天的数据，请将Kafka的保留时间设置为30天。2.当您要进行reprocessing时，启动流处理job的另一个实例，该job从保留数据的开始处开始处理，但将此输出数据定向到新的输出表。3.当第二个job完成时，将应用程序切换为从新表读取。4.停止旧版本的job，并删除旧的输出表。 那30天以前的数据呢？就不管了吗与Lambda架构不同，在这种方法中，您只在处理代码更改时进行reprocessing并重新计算结果。当然，重新计算的job只是相同代码的改进版本，运行在相同的框架上，使用相同的输入数据。您会希望提高reprocessing job的并行性，以便它能够非常快地完成。 也许我们可以称它为Kappa架构，尽管它太简单以至于只值得一个希腊字母。 撤回功能当然，您可以进一步优化它。一般情况下，您可以合并两个输出表。Jay Kreps认为有必要在短时间内同时拥有两者。这允许您通过一个将应用程序重定向到旧表的按钮来立即恢复到旧逻辑（撤回）。在特别重要的情况下(比如，您的广告目标标准)，您可以使用自动A/B测试或bandit算法来控制切换，以确保与之前的系统相比，您正在推出的任何bug修复或代码改进不会意外地降低性能。 HDFS和Kafka集成请注意，这并不意味着您的数据不能存到HDFS;这只是意味着你不会在那里进行reprocessing。Kafka与Hadoop有很好的集成，所以将任何Kafka主题镜像到HDFS中都很容易。可以将流处理作业的输出流甚至中间流用于Hadoop中的分析工具(如Hive)，或者用作其他离线数据处理流的输入。 我们已经使用Samza实现了此方法以及其他reprocessing体系结构的变种。 六、一些背景（Kafka）对于那些不太熟悉Kafka的人来说，我刚才所描述的可能没有意义。快速复习一下可能会把事情弄清楚。Kafka保持这样的有序日志:知道你不早说Kafka的“主题”是这些日志的集合： 使用此数据的流处理器只维护一个“偏移量”，即它在每个分区上处理的最后一条记录的日志条目号。因此，更改使用者的位置以返回并重新处理数据非常简单，只需使用不同的偏移量重新启动作业即可。为相同的数据添加第二个使用者只是 添加 另一个指向日志中不同位置的读取器。不管偏移量在哪，日志文件保持近三十天的内容不变，重启作业时，改作业顺着日志走到头。 Kafka支持复制和容错，运行在廉价的普通硬件上，并且很乐意在每台机器上存储很多TBs数据。因此，保留大量数据是一种非常自然和经济的做法，不会影响性能。LinkedIn在网上保留了pb级的Kafka数据，许多应用程序都很好地利用了这种长时间的保留模式。 廉价的消费和保留大量数据的能力使得添加第二个“reprocessing” job仅仅是启动代码的第二个实例，只是要从日志中的不同位置开始。那日志没有保存的操作就不用重新执行了吗刚好就很适合做流处理的reprocessing这个设计不是偶然的。我们构建Kafka的目的就是把它作为流处理的基础来使用，而且我们想要的正是进行reprocessing的模型。好奇的读者可以在这里找到更多关于卡夫卡的信息。 然而，从根本上说，没有任何东西把这个想法与Kafka联系起来，Kafka就是个日志系统而已。您可以用这个方法替换任何支持长期保留有序数据的系统（如HDFS）。实际上，很多人都熟悉类似的模式，即事件源或CQRS。当然，分布式数据库的人会告诉你，这只是实例化视图维护的一个rebranding，他们会很高兴地提醒你，他们很久以前就知道了，sonny。 七、比较Jay Kreps指出这种方法可以将Samza用作流处理系统，因为他们在LinkedIn上就是这么做的。但是Jay Kreps不知道它在Storm或其他流处理系统中不能同样有效的原因。“我对Storm不是很熟悉，所以我想知道其他人是否已经在这么做了”。无论如何，Jay Kreps认为总体思想是与系统无关的。是啊为什么呢 这两种方法之间的效率和资源权衡有点像wash。Lambda架构要求一直运行reprocessing和实时处理，而Jay Krep所建议的只需要在需要reprocessing时运行job的第二个副本。 但是，Jay Kreps的建议要求在输出数据库中暂时拥有2倍的存储空间，并且需要一个支持大容量写入的数据库来进行重新加载。在这两种情况下，重新处理的额外负荷可能会被平均化。如果您有很多这样的工作，它们将不会一次全部reprocess，因此在具有数十个这样的工作的共享群集上，您可以为可能会在任何给定时间进行reprocessing的少数工作预留额外的百分之几的容量。 Kafka真正的优势不在于效率，而在于允许人们在单一的处理框架上开发、测试、调试和操作他们的系统。因此，在简单性很重要的情况下，可以将此方法视为Lambda架构的替代方案。 启发维持两个系统同状态很难，那就只维持一个系统，另一个系统给它换成这个系统。为什么这个方法在Storm或其他流处理系统中不能同样有效有没有什么办法改进kappa对存储空间或大容量写入的要求替换方案不一定要全方位比原方法好，可以只考虑其中一个需求日志系统中删除的日志如何重计算 [1]Jay Kreps.Questioning the Lambda Architecture","categories":[],"tags":[{"name":"Lambda","slug":"Lambda","permalink":"http://yimial.github.io/tags/Lambda/"},{"name":"大数据","slug":"大数据","permalink":"http://yimial.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"The Lambda architecture——principles for architecting realtime Big Data systems阅读笔记（翻译）","slug":"Lambda detail","date":"2019-11-10T13:45:43.000Z","updated":"2020-07-09T14:50:12.999Z","comments":true,"path":"2019/11/10/Lambda detail/","link":"","permalink":"http://yimial.github.io/2019/11/10/Lambda%20detail/","excerpt":"","text":"query = function(all data)本文介绍了Lambda架构的三个层的任务和特点，最后就Lambda各个设定的意义展开了思考 一、引言Nathan Marz和James Warren合著的《大数据——可伸缩实时数据系统的原则和最佳实践》介绍了Lambda架构。Lambda是第一个真正将批处理和流处理协作解决无数用例的体系结构。 Lambda架构的前提：可以对所有数据运行ac-hoc查询来获得结果，但是这样做资源开销很大。 ac-hoc查询：即席查询是指那些用户在使用系统时，根据自己当时的需求定义的查询。 从技术上讲，现在对大数据运行ac-hoc查询(Cloudera Impala)是可行的，但在每次需要计算URL的页面访问量时查询pb级数据可能不是最有效的方法。因此，我们的想法是将结果预计算为一组视图，然后查询这些视图。 二、Lambda架构分为三个层次1.批处理层(Apache Hadoop)存储、计算 批处理层负责两件事。第一个是存储不可变的、不断增长的主数据集(HDFS)，第二个是从这个数据集计算任意视图(MapReduce)。 迭代计算 计算视图是一个持续的操作，当新数据到达时，它将在下一次MapReduce迭代的重计算过程中聚合到视图中。视图计算整个数据集，因此批处理层不需要频繁地更新视图。根据数据集和集群的大小，每次迭代可能需要几个小时。 2.服务层(Cloudera Impala)索引、Impala公开视图 批处理层的输出是一组包含预计算视图的平面文件。服务层负责索引和公开视图，以便查询。由于批处理视图是静态的，所以服务层只需要提供批量更新和随机读取功能，为此我们将使用Cloudera Impala。要使用Impala公开视图，服务层需要做的就是在Hive Metastore中创建一个指向HDFS文件的表（索引？）。用户将能够使用Impala立即查询视图。 12Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。Impala的定位是OLAP 引出速度层 Hadoop和Impala是批处理和服务层的完美工具。Hadoop可以存储和处理pb级的数据，Impala可以快速、交互式地查询这些数据。尽管如此，批处理和服务层本身并不满足任何实时需求，因为MapReduce(按设计)是高延迟的，可能需要几个小时才能在视图中表示新数据并将其传播到服务层。这就是为什么我们需要速度层。 请注意“实时”一词的用法。当作者说实时时，实际上作者指的是near-实时(NRT)，也就是事件发生与该事件的处理出任何数据之间的时间延迟。 12In the Lambda architecture, realtime is the ability to process the delta of data that has been captured after the start of the batch layers current iteration and now. 3.速度层增量，对应批处理层的迭代计算 本质上，速度层与批处理层相同，它也使用接收到的数据计算视图。但速度层需要补偿批处理层的高延迟，它通过在Storm中计算实时视图来达到这个目的。实时视图只包含对批处理视图进行补充的增量结果。批处理层从头开始不断地重新计算批处理视图，而速度层使用增量模型，当接收到新数据时，递增实时视图。 临时性，批处理层处理后即删除 临时性：speed层的聪明之处在于实时视图是瞬态的，一旦数据通过了批处理和服务层的处理，实时视图中的相应结果就可以丢弃。在书中，这被称为“复杂性隔离”，这意味着架构中最复杂的部分被推到结果是临时存在的层中。 Storm计算实时视图、HBase存储实时视图、Impala查询合并视图 最后一个难题是公开实时视图，以便对他们进行查询并与批处理视图合并，以获得完整的结果。由于实时视图是增量的，所以speed层需要随机读写，为此我们将使用Apache HBase。HBase为Storm提供了持续increment实时视图的能力，同时可以通过Impala查询与批处理视图的合并结果。Impala既可以查询存储在HDFS中的批处理视图，又可以查询存储在HBase中的实时视图，这使得它成为完成这项任务的完美工具。 三、思考Hadoop使得我们更方便地查询和更新数据。 不可变记录的意义 不可变记录（record）是一个记录在某一时刻的版本。可以添加记录的较新版本，但不能覆盖特定的版本，这意味着始终可以恢复到以前的状态。在Lambda架构中，这意味着如果您不小心添加了一些坏的记录，可以通过简单地删除这些记录，然后重新计算视图以修复问题。这本书把这个称为“human fault-tolerance” recompute的意义您可能会认为从头计算是一个bad idea，而且实现增量MapReduce算法来增加批处理视图更新的频率肯定会更有性能。尽管这样，你依然会牺牲性能来换取human fault-tolerance，因为在使用增量算法时，要修复视图中的任何问题都要困难得多。Storm层一般无法进行recompute，因为流处理层一般不保存数据，重现数据的增量计算很难 仅使用Hadoop生态系统能否实现相同的结果？（整个不懂，下次再学，总而言之兴许可以，但没必要）作者认为实现这种体系结构的原因在于您对实时的看法和需求，以及您是否认为human fault-tolerance是系统中的一个重要因素。在Hadoop中实现低延迟系统是可行的。例如，可以使用Apache Flume创建一个通到HDFS或HBase的ingest管道，并使用Impala查询数据。Flume的最新版本(1.2.0)还引入了拦截器的概念，可用于修改流数据。但Flume by design不是Storm那样的流媒体分析平台，因此作者认为很难在Flume中计算实时视图(但也不是不可能)。另一方面，Storm是一个专门构建的、可伸缩的流处理系统，它的延迟通常要低得多。 作者从大数据这本书(至少是前6章)中学到最多的是这个架构的原理。还有不变性和human fault-tolerance的重要性，以及预计算和重计算的好处。如果您正在考虑完整地实现Lambda架构，请问自己一个问题:我需要多实时?如果你的答案是在几十秒，那么完整的Lambda架构可能没有必要，但如果你的答案是毫秒，那么作者认为Lambda架构就是你的答案。 作为这篇文章的前导，作者一直在为Storm开发一个HBase连接器。连接器为在Lambda架构中创建实时视图提供了大量的Storm Bolt和Trident state的实现。 启发通过批处理视图迭代重计算+实时视图增量计算的方式，查询时拼接视图，使得系统架构可以更好的适用于这两种需求：满足容错性和实时性。 [1]James Kinley.The Lambda architecture——principles for architecting realtime Big Data systems[这篇博客的源博客的源博客：How to beat the CAP theorem]（http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）","categories":[],"tags":[{"name":"Lambda","slug":"Lambda","permalink":"http://yimial.github.io/tags/Lambda/"},{"name":"大数据处理框架","slug":"大数据处理框架","permalink":"http://yimial.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"}]}],"categories":[],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yimial.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"关系抽取","slug":"关系抽取","permalink":"http://yimial.github.io/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"},{"name":"实体识别","slug":"实体识别","permalink":"http://yimial.github.io/tags/%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"},{"name":"Lambda","slug":"Lambda","permalink":"http://yimial.github.io/tags/Lambda/"},{"name":"大数据","slug":"大数据","permalink":"http://yimial.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理框架","slug":"大数据处理框架","permalink":"http://yimial.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"}]}