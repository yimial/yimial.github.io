<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Jointly extracting relations with class ties via effective deep ranking论文笔记</title>
    <link href="/2020/07/14/classtie/"/>
    <url>/2020/07/14/classtie/</url>
    
    <content type="html"><![CDATA[<blockquote><p>在关系提取中，关系之间的联系是很常见的，本文称之为class tie。独立监督场景中，一个实体元组可能有多个关系事实。利用一个实体对的多个关系之间的class tie将是远程监督关系提取的一个前景。</p></blockquote><p><strong>摘要</strong></p><p>为了有效地利用class tie，本文提出用一个联合模型来完成联合关系提取，该模型集成了卷积神经网络(CNN)和一个通用的两两排名框架，其中引入了三个排名损失函数。</p><p>此外，还提出了一种有效的方法来缓解模型训练中出现的严重类平衡问题。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>class tie是指关系提取中关系之间的联系，它有两种类型:弱class tie和强class tie。弱class tie主要包括出生地点和居住地，公司的总裁和创始人等关系之间的联系。相反，强class tie意味着关系之间潜在的逻辑蕴含关系，如首都-&gt;城市，而反过来是不成立的。</p><p>如“拿琼贝尼的话来说，她的母亲帕齐·拉姆齐一出生就从未离开过亚特兰大”，这句话中有两个关系，一个是出生地，一个是居住地，有关居住地的信息很难提取，但如果能结合出生地和居住地之间的关系，那么抽取出生地的过程将为居住地的抽取提供信息。</p><p><strong>class tie对远程监督关系抽取非常重要。</strong>一个实体对可能包含多个事实关系，可以利用class tie来增强关系提取，以便在预测未知关系时缩小潜在的搜索空间和减少关系之间的不确定性。</p><p>为了挖掘关系之间的class tie，本文提出对一个实体对中的所有正标签进行联合提取，并考虑正标签和负标签之间的两两连接。通过联合提取两个关系，学习到训练样本中这两个关系的class tie，然后利用这个学习信息从未知关系的实例提取那些由于缺乏信息而无法单独提取出的关系。</p><p>两两排名损失利用正标签与负标签之间的两两联系，采用两两排名的方法使正标签的排名高于负标签，将正标签从负标签中分离出来。</p><p>此外，<strong>结合句子之间的信息</strong>更适合进行联合提取，这样可以从其他句子中获得更多的信息来提取每个关系。</p><h4 id="综上所述"><a href="#综上所述" class="headerlink" title="综上所述"></a>综上所述</h4><p>本文提出了一个统一的模型，将<strong>两两排名与CNN相结合，以挖掘class tie</strong>。受深度学习对句子建模有效性的启发，本文使用CNN对句子进行编码。类似(Santos等人，2015;Lin等人，2016)，本文使用类嵌入来表示关系类型。</p><p>本文引入了两种不同的方法将多个句子组合成一个<strong>包表示向量</strong>，目的是<strong>在句子之间聚合信息</strong>，然后度量了包表示向量与关系类型嵌入在实值空间中的相似性。针对组合句子的两种方法，提出了三种新的两两排名损失函数进行正标签联合提取。</p><p>此外，为了缓解NR (not relation)在训练模型时对<strong>类别平衡的不良影响</strong>，设计在训练过程中<strong>减少NR对损失传播的影响</strong>。</p><p>本文的贡献可以概括如下:</p><ul><li>提出利用class tie来增强关系提取。</li><li>利用深度排名来自动学习class tie，引入了一种有效的深度排名模型，该模型将CNN和两两排名框架相结合，以挖掘class tie。</li><li>提出了一种有效的方法来缓解NR数据不平衡对模型训练的影响。</li></ul><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>Hoffmann和surdeanu通过多实例多标签学习对关系抽取建模，提取重叠关系。虽然他们也提出对关系进行联合提取，但他们只使用单个句子的信息，而不使用其他句子的信息。Han和Sun(2016)尝试使用马尔科夫逻辑模型来捕捉关系标签之间的一致性</p><p>深度学习排名作为一种分类模型被广泛应用于许多问题中。在图像检索中，Zhao等(2015)将深度语义排名应用于多标签图像检索。在文本匹配中，Severyn和Moschitti(2015)采用排名结合deep CNN进行短文本对匹配。在传统的监督关系提取中，Santos等人(2015)设计了一种基于CNN的两两损失函数来提取单标签关系。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h4 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h4><p>定义关系类型 L={1，2，…, C}</p><p>实体对 T = {t<sub>i</sub>}<sub>i=1</sub><sup>M</sup>，</p><p>对齐样本 X={x<sub>i</sub>}<sub>i=1</sub><sup>N</sup>，</p><p>构造数据集：D={（t<sub>i</sub>，L<sub>i</sub>，X<sub>i</sub>）}<sub>i=1</sub><sup>H</sup>，t<sub>i</sub>实体对拥有关系集L<sub>i</sub>，样本集X<sub>i</sub>提到了实体对t<sub>i</sub>。</p><p>给定一个数据（t<sub>k</sub>，L<sub>k</sub>，X<sub>k</sub>），将X<sub>k</sub>由CNN进行编码后将其表示定义为S<sub>k</sub>={s<sub>i</sub>}<sub>i=1</sub><sup>|Xk|</sup></p><p>关系embedding W∈ R<sup>|L|×d</sup>表示关系类型</p><h3 id="使用卷积层计算句子表示"><a href="#使用卷积层计算句子表示" class="headerlink" title="使用卷积层计算句子表示"></a>使用卷积层计算句子表示</h3><h4 id="单词表示"><a href="#单词表示" class="headerlink" title="单词表示"></a>单词表示</h4><ul><li><p>word embedding</p></li><li><p>position embedding</p></li></ul><p>结果：d<sub>w</sub> = d<sub>1</sub> + d<sub>2</sub> ×2</p><h4 id="多核卷积和分段最大池化"><a href="#多核卷积和分段最大池化" class="headerlink" title="多核卷积和分段最大池化"></a>多核卷积和分段最大池化</h4><p>结果o ∈ R<sup>ds∗3</sup></p><p>d<sub>s</sub>是卷积核数量</p><h4 id="非线性层和dropout层"><a href="#非线性层和dropout层" class="headerlink" title="非线性层和dropout层"></a>非线性层和dropout层</h4><p>s = g(o)◦h </p><p>g是非线性函数，本文使用tanh；h是一个伯努利随机向量，p的概率是1。</p><p>此处顺序和之前的Zeng、Jiang等人的研究不同</p><h3 id="使用联合抽取和两两排名损失学习class-tie"><a href="#使用联合抽取和两两排名损失学习class-tie" class="headerlink" title="使用联合抽取和两两排名损失学习class tie"></a>使用联合抽取和两两排名损失学习class tie</h3><p>如上所述，为了学习class tie，本文提出在考虑正标签和负标签两两连接的情况下进行联合提取。为了实现这一目标，本文应用了两两排名。</p><p>此外，为了进行联合提取，还需要结合句子之间的信息。</p><p><img src="/imgs/2020071401/%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" alt="流程"></p><p>具体来说，从下到上，预先传播句子集中的所有信息，为联合提取提供足够的信息；由上至下，结合两两排名损失，联合提取正标签关系，将损失反向传播到CNN，来学习class tie。</p><h4 id="结合句子间信息"><a href="#结合句子间信息" class="headerlink" title="结合句子间信息"></a>结合句子间信息</h4><p>本文提出了两种组合一个包中句子的方法，为联合抽取提供足够的信息。</p><p><strong>第一种方法是平均法</strong>。该方法对所有的句子一视同仁，直接对句子embedding各个维度的值取平均</p><p><img src="/imgs/2020071401/%E5%B9%B3%E5%9D%87%E6%B3%95.png" srcset="/img/loading.gif" alt="平均法"></p><p>其中n为句子数量，s为结合了所有句子嵌入的表示向量。</p><p>由于该方法对所有句子的重要性赋予相等的权重，因此可能会从两个方面带来大量的噪声数据:</p><ul><li><p>错误的标注数据</p></li><li><p>与一个关系类型不相关的对齐样例，因为所有包含相同实体对的句子都会被用于联合构建这个包的表示</p></li></ul><p><strong>第二种是Lin等人(2016)使用的句子级注意力算法</strong>，用来衡量句子的重要性，以缓解标注错误的问题。对于每一个句子，ATT都要通过将该句子与关系类别进行比较来计算权重。</p><p>首先计算一个句子embedding和关系类别之间的相似性</p><p><img src="/imgs/2020071401/ATT%E7%9B%B8%E4%BC%BC%E6%80%A7.png" srcset="/img/loading.gif" alt="ATT相似性"></p><p>e<sub>j</sub>代表句子embedding s<sub>j</sub>和关系类别c之间的相似度，然后使用softmax操作将e缩放到[0,1]</p><p><img src="/imgs/2020071401/ATTsoftmax.png" srcset="/img/loading.gif" alt="ATTsoftmax"></p><p>最后使用如下公式合并s：</p><p><img src="/imgs/2020071401/ATT%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.png" srcset="/img/loading.gif" alt="ATT加权平均"></p><h4 id="结合损失来学习class-tie的联合提取"><a href="#结合损失来学习class-tie的联合提取" class="headerlink" title="结合损失来学习class tie的联合提取"></a>结合损失来学习class tie的联合提取</h4><p><strong>分数函数</strong></p><p>首先，给出分数函数来衡量s和关系c之间的相似性。本文使用点积函数来计算把句子s预测为c类的得分。</p><p><img src="/imgs/2020071401/%E5%BE%97%E5%88%86%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" alt="得分函数"></p><p>分数函数还有其他选择。Wang等人(2016)提出了一种基于边际的损失函数，通过距离度量s和W<sub>[c]</sub>之间的相似性。由于分数函数在本模型中不是一个重要的问题，于是采用Santos等人(2015)和Lin等人(2016)使用的点积函数作为本模型的分数函数。</p><p><strong>排名损失函数</strong></p><p>两两排名的目的是学习分数函数F(s,c)，以使正标签的排名高于负标签。这个目标可以总结如下</p><p><img src="/imgs/2020071401/%E6%8E%92%E5%90%8D%E6%8D%9F%E5%A4%B1%E7%9B%AE%E6%A0%87.png" srcset="/img/loading.gif" alt="排名损失目标"></p><p>其中，β是控制正标签得分和负标签得分之间的最小边际的边缘因子</p><p>为了关系之间的class tie，本文扩展该目标，使用联合抽取并提出了三个带有组合句变体的排序损失函数。</p><ul><li>with AVE：使用平均法聚合句子时的选择</li></ul><p><img src="/imgs/2020071401/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B01.png" srcset="/img/loading.gif" alt="损失函数1"></p><p>​    σ+是正边界，σ-是负边界。根据 σ+ − σ−的大小控制正标签的排名高于负标签。实际上，F(s,c<sup>+</sup>)比正边界高，F(s,c<sup>-</sup>)比负边界低。</p><p>​    采用基于mini-bach的随机梯度下降(SGD)来最小化损失函数。选择负类中得分最高的类作为负类c-</p><p><img src="/imgs/2020071401/c-1.png" srcset="/img/loading.gif" alt="c-1"></p><p>​    在每一轮训练中更新一个负类。为了平衡正类和负类之间的损失，在函数的右项前乘以|Lk|，以扩大负类损失</p><ul><li>with ATT：使用注意力法聚合句子时的选择</li></ul><p><img src="/imgs/2020071401/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B02.png" srcset="/img/loading.gif" alt="损失函数2"></p><p>​    s<sup>c</sup>表示s的注意力权重是由将句子embedding和关系c对比相似度后加权混合得到的。c<sup>- </sup>由以下公式决定，这意味着每一轮训练都更新一个负类</p><p><img src="/imgs/2020071401/c-2.png" srcset="/img/loading.gif" alt="c-2"></p><p>​    对每个正类c<sup>+</sup>，这种方法捕捉多个句子中与类别c<sup>+</sup>最相关的信息来构建s<sup>c<sup>+</sup></sup>，然后控制F(s<sup>c<sup>+</sup></sup> ,c<sup>+</sup> )的排名比F(s<sup>c<sup>+</sup></sup> ,c<sup>-</sup> )高。</p><ul><li>ATT的扩展</li></ul><p>​    根据第二种目标函数,对于每一个c+,前两种方法只选择一个负类来更新参数,它只考虑正类和负类之间的联系,忽视正类之间的联系,所以第三种方法对第二种目标函数进行扩展，通过考虑正类之间的连接关系更好地利用class tie。</p><p><img src="/imgs/2020071401/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B03.png" srcset="/img/loading.gif" alt="损失函数3"></p><p>​    c<sup>-</sup>的选择</p><p><img src="/imgs/2020071401/c-3.png" srcset="/img/loading.gif" alt="c-3"></p><p>​    s<sup>c*</sup>是根据句子embedding和L中某个c*类的相似度计算的包向量表示，用该表示和所有c<sup>+</sup>类计算得分。从而根据s<sup>c*</sup>更新其他正类的关系类别embedding。通过这种方式，本文的模型可以捕捉和学习正类之间的联系。</p><p>以上三种方法，将一对实体对的所有正标签的loss合并起来进行联合提取（根据实体对划分包），以捕获关系之间的类关联。如果分开进行提取，多个正标签的损失将被分离，与联合提取相比，将得不到足够的正标签之间的关联信息。正标签和正标签之间的关系通过ATT的扩展和正标签联合抽取进行学习，正标签和负标签之间的关系通过σ<sup>+</sup> − σ<sup>−</sup>学习。</p><h3 id="缓解NR的影响"><a href="#缓解NR的影响" class="headerlink" title="缓解NR的影响"></a>缓解NR的影响</h3><p>在关系提取中，数据集总是包含某些不包含关系的分类为NR(not relation)的负样本。数据集中，这种负样本的比例可能非常高，而数据不平衡会严重影响模型训练，导致模型只对比例较高的类别敏感。</p><p>在基于DS的关系提取中，为了减轻NR的影响，本文减少了NR的损失传播，即当关系c为NR时，将其损失设为0。但并不省略NR类的关系类别embedding。相反，如果使用ATT方法进行句子间的信息组合，NR类的关系类别embedding将根据负类的损失（公式的右部分）进行更新。</p><p>损失函数变种3的伪代码：</p><p><img src="/imgs/2020071401/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B03%E4%BC%AA%E4%BB%A3%E7%A0%81.png" srcset="/img/loading.gif" alt="损失函数3伪代码"></p><h3 id="其他设置"><a href="#其他设置" class="headerlink" title="其他设置"></a>其他设置</h3><p>使用一个word2vec工具 gensim 3在NYT语料库中训练单词嵌入，保留出现超过100次的单词，构建单词词典，用“UNK”来表示其他单词。</p>]]></content>
    
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>远程监督</tag>
      
      <tag>多实例</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Relation Extraction with Multi-instance Multi-label Convolutional Neural Networks论文笔记</title>
    <link href="/2020/07/13/MIMLDS/"/>
    <url>/2020/07/13/MIMLDS/</url>
    
    <content type="html"><![CDATA[<blockquote><p>该论文在Zeng提出的远程监督多实例PCNN (Zeng et al., 2015)上做出了改进。</p></blockquote><p><strong>Zeng方法的两个缺点</strong>：</p><ul><li>基于expressed-at-least-once假设。这个假设为：当两个实体存在某种关系，那么至少有一个提到这两个实体的样本表达该关系。这个假设过于强烈，根据这个假设，PCNN在训练和预测时，只选择每个实体对的包中最有可能的句子，这样可能会失去其他句子所包含的丰富信息。且实际上，给定两个参与一个关系的实体，可能很难从训练文本中找到表达该关系的确切的单个句子。通过聚合多个句子中可用的信息，可能会得到更多支持事实的信息。</li><li>其次，PCNN将远监督RE视为一个单标签学习问题，为每个实体对选择一个关系标签，忽略了同一实体对之间可能存在多个关系的事实。</li></ul><p>针对以上问题，本文提出了一个多实例多标签卷积神经网络(MIMLCNN)架构来解决上述两个问题。</p><ul><li>首先，放松了expressed-at-least-once假设，而是假设两个实体之间的关系可以从提到这两个实体的所有句子中显式地表达或隐式地推导出来。因此，在使用卷积架构自动提取每个句子内的特征后，使用跨句子最大池化来选择不同句子中的特征，然后将最显著的特征聚合为每个实体对的向量表示。从而充分利用了这些句子中所包含的所有可用信息。</li><li>对于第二个问题，本文通过在神经网络分类器中设计不同的多标签损失函数来处理重叠关系。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>该模型以实体对(e<sub>1</sub>, e<sub>2</sub>)以及与实体对对齐的所有句子作为输入，输出两个实体之间保持的知识库关系集。</p><p><img src="/imgs/2020071301/%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" alt="模型"></p><p>关键步骤:(1)句子级特征提取 (2)跨句子最大池化 (3)多标签关系建模</p><h3 id="句子级特征提取"><a href="#句子级特征提取" class="headerlink" title="句子级特征提取"></a>句子级特征提取</h3><p>目的是为每一个对齐的句子生成一个特征向量。首先将句子长度填充到h，并将其转换为一个矩阵表示，其中每一行表示一个单词标记。然后对矩阵进行卷积、分段最大池化运算，得到向量表示。</p><p>其中单词嵌入在训练数据集上运行skip-gram模型得到，位置嵌入使用[-1,1]的均匀分布随机初始化</p><p><img src="/imgs/2020071301/%E5%8F%A5%E5%AD%90%E7%BA%A7%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96.png" srcset="/img/loading.gif" alt="句子级特征提取"></p><p>使用和Zeng一样的embedding层，X ∈ R<sup>h×d<sub>s</sub></sup>，d<sub>s</sub> = d<sub>w</sub> + 2 ∗ d<sub>p</sub> ，h是句子长度。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积操作表达式如下</p><p><img src="/imgs/2020071301/%E5%8D%B7%E7%A7%AF%E5%B1%82.png" srcset="/img/loading.gif" alt="卷积层"></p><p>W = R<sup>w<sub>c</sub> ×d<sub>s</sub></sup>是卷积矩阵，w<sub>c</sub>是窗口大小，f是非线性函数如tanh，使用不同的W和b构造不同的卷积核，得到句子的n个特征向量。然后将得到的特征进行叠加以构建矩阵 C ∈ R<sup>n×(h−w<sub>c</sub> +1) </sup></p><h3 id="分段最大池化"><a href="#分段最大池化" class="headerlink" title="分段最大池化"></a>分段最大池化</h3><p>和Zeng一样，分段池化后p ∈ R<sup>3n</sup></p><p><img src="/imgs/2020071301/maxpooling.png" srcset="/img/loading.gif" alt="maxpooling"></p><h3 id="句子间最大池化"><a href="#句子间最大池化" class="headerlink" title="句子间最大池化"></a>句子间最大池化</h3><p>本文放松expressed-at-least-once的假设，充分利用句子间的信息。</p><p>新假设：两个实体之间的关系可以明示或隐式地从所有提到这两个实体的句子中推导出来。</p><p>该假设允许从所有相关句子的信息中推理出预测结果。由于这种假设的性质，本文选择跳过句子级的关系提取，直接在实体对级进行预测。</p><p>本文提出使用跨句最大池化来利用这一假设。假设有m个与实体对对齐的句子，p<sub>i</sub><sup>j</sup>表示第j个句子的向量表示的第i个元素。</p><p><img src="/imgs/2020071301/%E8%B7%A8%E5%8F%A5%E5%AD%90%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96.png" srcset="/img/loading.gif" alt="跨句子最大池化"></p><p>跨句子最大池化将所有句子级表示聚合为实体对级表示g = [g<sub>1</sub> ,g<sub>2</sub> , … ,g<sub>3n</sub>]，该操作</p><ul><li><p>支持直接提取实体对级关系</p></li><li><p>可以从不同的句子中收集信息，使分类器能够利用来自不同句子的信息进行预测</p></li></ul><p>其他方法，如平均池，也可以应用于这个短语，但使用跨句最大池的原因如下:</p><ul><li><p>在实体对级关系提取中，一个特征的多次出现不会提供更多的额外信息。也就是说，一个仅出现一次的特征信号也足以提取关系。这种思维体现在跨句最大池化操作中，即跨句收集每个特征的最大激活水平。</p></li><li><p>相比之下，平均池化是根据句子的数量来计算激活信号的平均值，因此在多次提到的实体对的表示中，用于预测的特征信号可能会被稀释。</p></li></ul><h3 id="多标签关系模型"><a href="#多标签关系模型" class="headerlink" title="多标签关系模型"></a>多标签关系模型</h3><p>首先计算每个标签的置信值</p><p><img src="/imgs/2020071301/%E7%BD%AE%E4%BF%A1%E5%80%BC.png" srcset="/img/loading.gif" alt="置信值"></p><p>再对分数向量o的每个元素应用sigmoid函数，计算各关系的概率。其中M为对齐语句的集合，l为关系标签的数量</p><p><img src="/imgs/2020071301/sigmoid.png" srcset="/img/loading.gif" alt="sigmoid"></p><p>使用二值标签向量y表示实体对之间的真实关系的集合，其中1表示该集合中存在的关系，否则为0。这样，NA(表示实体对之间没有关系)自然地被表示为一个全零向量。</p><p>预测时，给定一个实体对，该模型选择概率超过0.5的关系作为预测标签。如果没有这样的关系，则将NA赋给这个实体对。</p><p>值得注意的是，关系往往不是独立的。例如，如果triple (A, capital, B)存在，另一个triple (A, contains, B)也存在。在本文模型中，通过对所有关系标签使用共享的实体对级表示来处理关系之间的依赖关系。也就是说在分包时不考虑关系类型是否相同，只考虑实体对中的两个实体。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>本文设计了两个多标签建模损失函数，y<sub>i</sub>是标签 i 的真实值:</p><p><img src="/imgs/2020071301/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B01.png" srcset="/img/loading.gif" alt="损失函数1"></p><p><img src="/imgs/2020071301/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B02.png" srcset="/img/loading.gif" alt="损失函数2"></p><h3 id="其他设置"><a href="#其他设置" class="headerlink" title="其他设置"></a>其他设置</h3><p>损失函数采用Adadelta</p><p>对倒数第二层采用dropout（g中的每一个元素都是通过乘以一个概率p为0的伯努利随机变量随机退出的）</p><p>在测试时，在评分前将学习矩阵w<sub>1</sub>按p(即w<sub>1</sub> = pw<sub>1</sub>)进行缩放</p><p>为了方便比较，参数设置和Zeng的PCNN基本相同</p>]]></content>
    
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>远程监督</tag>
      
      <tag>多实例</tag>
      
      <tag>多标签</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks论文笔记</title>
    <link href="/2020/07/12/PCNNMIL/"/>
    <url>/2020/07/12/PCNNMIL/</url>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>传统远程监督两大问题：</p><ul><li>远距离监管的假设过于强烈，导致错误的标签问题。一个句子提及知识库中的两个实体并不一定表示了它们之间的关系，这两个实体可能只是共享相同的主题。一个句子没有表达这种关系，但仍然被选为训练实例。这将会阻碍在这种噪声数据上训练的模型的性能。</li><li>NLP工具的误差导致错误传播或积累。而且句子越长，准确性越低，而来自web的句子一般都很长。</li></ul><p>解决方案：提出了一个新的模型，称为多实例学习的分段卷积神经网络</p><ul><li>将远程监督关系提取作为一个多实例问题，考虑了实例标签的不确定性。在多实例问题中，训练集由多个包组成，每个包包含多个样本。包的标签是已知的;然而，包内的样本的标签是未知的。在包层设计了一个目标函数。在学习过程中，考虑了样本标签的不确定性；这缓解了错误的标签问题。</li><li>避免了特征工程，而是采用卷积架构和分段最大池化来自动学习相关的特征。对Zeng的CNN关系抽取网络进行扩展，使用最大池化操作来确定最显著的特征。</li><li>单个最大池化不足以捕捉两个实体之间的结构信息。为了捕获结构和其他潜在信息，根据两个给定实体的位置将卷积结果分成三段，并设计了分段最大池化层，分段最大池化层返回每个片段中的最大值。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型分为四层：向量表示层、卷积层、分段最大池化层和softmax输出层。</p><h3 id="向量表示"><a href="#向量表示" class="headerlink" title="向量表示"></a>向量表示</h3><p>通过查找预先训练好的单词嵌入，每个输入单词被转换成一个实值向量。此外，利用位置特征(PFs)来指定实体对，并通过查找位置嵌入来将位置特征转化为向量。</p><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>将文本中的每个单词映射到一个k维实值向量，它们能够很好地捕获单词的语义和句法信息。训练神经网络的常用方法是随机初始化所有参数，然后使用优化算法对其进行优化。最近的研究表明，当用单词嵌入初始化神经网络时，神经网络可以收敛到更好的局部极小值。单词嵌入通常是通过利用未标记文本中的单词的共现结构以完全无监督的方式学习的。使用预训练过的单词嵌入已经成为增强许多其他NLP任务的常见做法。本文使用 Skip-gram 训练单词嵌入。</p><h4 id="位置嵌入"><a href="#位置嵌入" class="headerlink" title="位置嵌入"></a>位置嵌入</h4><p>PF定义为当前单词到e1和e2的相对距离的组合。</p><p>随机初始化两个位置嵌入矩阵(pf1和pf2)。然后通过查找位置嵌入矩阵将相对距离转换为实值向量。  </p><p>在组合单词嵌入和位置嵌入时，向量表示部分将实例转换为S∈R<sup>S×d</sup>的矩阵，其中S是句子长度，d = d<sub>w</sub> + d<sub>p</sub>∗2。矩阵S随后被输入到卷积部分。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>在关系抽取中，标记了目标实体的输入句子只对应于一个关系类型。因此，可能需要利用所有的局部特征，在全局范围内进行预测。在神经网络中，卷积方法是自然适用于所有这些特征的方法。因此，本文利用卷积神经网络自动提取特征。</p><p><img src="/imgs/2020071201/%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" alt="模型"></p><p>为了获得多粒度信息，使用多个尺寸的卷积核(W = {w<sub>1</sub> ,w<sub>2</sub> ,··· ,w<sub>n</sub> })。</p><p>c<sub>i </sub>表示以 w<sub>i</sub> 为卷积核的输出，n代表卷积核的数量</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>卷积输出矩阵C的大小取决于句子的长度，一般将maxpooling层用于解决这个问题，其思想是在每个特征图中捕捉最重要的特征。</p><p>单个最大池化减少隐藏层大小的过程太快、太粗糙，无法捕获用于提取关系的细粒度特征。且不足以覆盖两个实体之间的结构信息。</p><p>因此本文提出分段最大池化，根据所选择的两个实体，将输入的句子分为三个部分，返回每个段中的最大值。</p><p><img src="/imgs/2020071201/%E5%88%86%E6%AE%B5%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96.png" srcset="/img/loading.gif" alt="分段最大池化"></p><p>i代表第i个卷积核，j代表句子的第j段，p<sub>ij</sub>代表句子第j段在第i个卷积核的输出，将每个卷积核的三段输出拼接。于是，对每个卷积核的输出，都能得到一个三维向量pi。最后拼接所有p1:n，n表示卷积核数量。</p><p><img src="/imgs/2020071201/tanh.png" srcset="/img/loading.gif" alt="tanh"></p><p>再连接到一个非线性函数如tanh，最后，分段最大池化层输出一个向量g。g ∈ R<sup>3n</sup>。</p><h3 id="softmax层"><a href="#softmax层" class="headerlink" title="softmax层"></a>softmax层</h3><p>为了计算每个关系的置信度，将特征向量g输入到softmax分类器中。</p><p>首先经过一个线性变换，W<sub>1</sub>∈ R<sup>n1×3n</sup>是变换矩阵，n1是关系类型数量。</p><p><img src="/imgs/2020071201/W1.png" srcset="/img/loading.gif" alt="W1"></p><p>测试时，使用缩放因子p对W<sub>1</sub>进行缩放W<sub>1</sub> = pW<sub>1</sub></p><p>前向推理时，给定一个输入实例m，带有参数的网络输出一个向量o，其中第r个分量o对应关系r的得分。为了获得所有关系类型的条件概率，对所有关系类型应用softmax操作。m<sub>i</sub><sup>j</sup>是第i个包的第j个样例。</p><p><img src="/imgs/2020071201/softmax.png" srcset="/img/loading.gif" alt="softmax"></p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout通过在前向计算过程中以概率p随机删除一个隐藏单元来防止隐藏单元的协同适应。</p><p>本文在倒数第二层使用dropout：对g应用“掩蔽”操作(g◦r)，其中r是一个概率p为1的伯努利随机向量。</p><p><img src="/imgs/2020071201/dropout.png" srcset="/img/loading.gif" alt="dropout"></p><h2 id="多实例"><a href="#多实例" class="headerlink" title="多实例"></a>多实例</h2><p>为了解决标记错误问题，本文对PCNNs进行多实例学习，在包级别上定义目标函数</p><p><img src="/imgs/2020071201/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" alt="目标函数"></p><p>其中j是这个包中类型为i的概率最大的样例</p><p><img src="/imgs/2020071201/j.png" srcset="/img/loading.gif" alt="j"></p><p>总体思想是：让‘每个包中最可能表达这个包所属关系类别的那个样例’的关系类别为i的概率最大</p><p>传统的反向传播算法是根据所有的训练实例来修改网络，而多实例学习的反向传播算法是根据包来修改网络。因此，本文的方法解决了远程监督关系提取的本质问题，也就是其中一些训练实例会不可避免地被错误标记。</p><p><strong>实现方法</strong>：对每一个minbatch，</p><ol><li>按照关系分类分成n个包，每个包一个一个输入网络。</li><li>计算包中所有样本的关系i的概率，求得该概率最大的样本j，将每个包符合该条件的样本抽出构建一个新的小样本集</li><li>用新构建出的样本集更新参数</li></ol><p>测试时每个样本正常通过PCNNs网络。</p>]]></content>
    
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>远程监督</tag>
      
      <tag>多实例</tag>
      
      <tag>PCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>End to end relation extraction using lstms on sequences and tree structures论文笔记</title>
    <link href="/2020/07/11/SPtree/"/>
    <url>/2020/07/11/SPtree/</url>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>在双向序列LSTM-RNNs上叠加双向树结构的LSTM-RNNs来捕获单词序列和依存树子结构信息<br>在单个模型中使用共享参数联合表示实体和关系<br>在训练过程中发现实体，并通过实体预训练和计划抽样等增强手法在关系抽取中使用实体信息</p><p>本文提出了一种基于神经网络的端到端模型来提取词序列和依赖树结构上实体之间的关系。通过使用双向序列(从左到右和从右到左)和双向树结构(自底向上和自顶向下)LSTM RNNs，在单个模型中对实体和关系进行联合建模。<br>本文模型在训练中进一步加入了两个增强:实体预训练和计划抽样。这些增强减轻了在训练早期阶段实体检测的性能低下的问题，并允许实体信息进一步帮助下游的关系分类</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/imgs/2020071101/%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" alt="模型"></p><p>该模型有三个主要网络层，依赖层叠加在序列层上，将单词序列和依存树结构的信息合并并输出；嵌入层和序列层由实体检测和关系分类任务共享。</p><p>在解码过程中，本文在序列层上建立贪心的、从左到右的实体检测，并在依赖层上实现关系分类，其中每个子树对应于两个被检测实体之间的一个关系候选。解码整个模型结构之后，通过时间反向传播(BPTT)同时更新参数。共享参数受实体标签和关系标签共同影响。</p><h3 id="单词嵌入层-embeddings-layer，即嵌入层"><a href="#单词嵌入层-embeddings-layer，即嵌入层" class="headerlink" title="单词嵌入层(embeddings layer，即嵌入层)"></a>单词嵌入层(embeddings layer，即嵌入层)</h3><p>该层使用v(w),v(p),v(d)和v(e)分别表示word、pos、依赖类型、实体标签的embedding向量</p><h3 id="基于单词序列的LSTM层-sequence-layer，即序列层"><a href="#基于单词序列的LSTM层-sequence-layer，即序列层" class="headerlink" title="基于单词序列的LSTM层(sequence layer，即序列层)"></a>基于单词序列的LSTM层(sequence layer，即序列层)</h3><p>双向序列LSTM，捕获句子的单词序列信息</p><p>第t个单词的LSTM单元表示如下：</p><ul><li><p>输入向量：x<sub>t</sub>=[v<sub>t</sub><sup>(w)</sup>;v<sub>t</sub><sup>(p)</sup>]，v<sub>t</sub><sup>(w)</sup>、v<sub>t</sub><sup>(p)</sup>分别表示单词、词性类型的embedding向量</p></li><li><p>h<sub>t-1</sub>：上一个隐层状态向量，c<sub>t-1</sub>：上一步的记忆单元向量</p></li><li><p>输出向量：s<sub>t</sub>=[→h<sub>t</sub>;←h<sub>t</sub>]。把两个方向LSTM的隐层状态进行拼接，交给下一层</p></li></ul><p><img src="/imgs/2020071101/LSTM.png" srcset="/img/loading.gif" alt="LSTM"></p><h3 id="实体检测："><a href="#实体检测：" class="headerlink" title="实体检测："></a>实体检测：</h3><p>在双向序列LSTM的顶部连接一个两层的神经网络，采用BILOU标注方案进行贪心的、从左到右的实体检测。</p><p>在这个解码过程中，使用一个单词的预测标签来预测下一个单词的标签。顶部的神经网路接收“序列层的相应输出和前一个单词的label embedding的拼接”</p><ul><li>W：权重矩阵</li><li>b：偏置向量</li></ul><p><img src="/imgs/2020071101/%E5%AE%9E%E4%BD%93%E6%A3%80%E6%B5%8B.png" srcset="/img/loading.gif" alt="实体检测"></p><h3 id="基于依赖子树的LSTM层-dependency-layer，即依赖层"><a href="#基于依赖子树的LSTM层-dependency-layer，即依赖层" class="headerlink" title="基于依赖子树的LSTM层(dependency layer，即依赖层)"></a>基于依赖子树的LSTM层(dependency layer，即依赖层)</h3><p>双向树结构LSTM，捕获句子的依赖树结构信息，沿候选实体对的最短依存路径构建</p><p>关系分类会使用到树的底部附近的参数节点，而自顶向下LSTM-RNN将从树的顶部向此类近叶节点发送信息</p><p>提出了一种新的树结构LSTM-RNN的变种，其中相同类型的子节点共享权矩阵Us。</p><p>第t个单词的LSTM单元计算公式如下：</p><ul><li>输入向量：x<sub>(d)</sub>=[s<sub>t</sub>; v<sub>t</sub><sup>(d)</sup>; v<sub>t</sub><sup>(e)</sup>]，s<sub>t</sub>表示t节点在序列层的相关隐状态向量，v<sub>t</sub><sup>(d)</sup>表示t节点到父节点的单词依赖类型，v<sub>t</sub><sup>(e)</sup>表示预测实体标签的embedding向量</li><li>C(t)：t节点的子节点集</li><li>m(·)：判断节点是否在关系的最短依存路径上的映射函数</li><li>h<sub>tl</sub>：子节点 l 的隐状态向量</li><li>f<sub>tk</sub>：当前节点对子节点 k 的遗忘门，U根据 k 和 l 决定</li><li>c<sub>tl</sub>：子节点 l 的记忆单元向量</li><li>输出向量：d<sub>p</sub>=[↑h<sub>pA</sub>;↓h<sub>p1</sub>;↓h<sub>p2</sub>]，把根节点和两个叶子节点的隐状态向量进行拼接（↑h<sub>pA</sub>是自底向上LSTM的最顶层的隐向量，表示目标词对的最低共同祖先；↓h<sub>p1</sub>,↓h<sub>p2</sub>是自顶向下的LSTM中两个目标词对应的LSTM单元的隐向量）</li></ul><p><img src="/imgs/2020071101/%E4%BE%9D%E8%B5%96%E5%B1%82LSTM.png" srcset="/img/loading.gif" alt="依赖层LSTM"></p><p>树结构的LSTM单元的计算借鉴了：</p><ul><li>Child-Sum Tree-LSTM：用子节点集的h的加权和作为本节点的h，每个子节点拥有自己的遗忘门f</li><li>N-ary Tree-LSTM：每个子节点有自己的权重矩阵U</li></ul><p>为了表示两个目标词对之间的关系，本文使用三种树结构进行了实验；</p><ul><li><p>SP-Tree：最短路径结构，捕获目标单词间的核心依赖路径。（一种节点类型）</p></li><li><p>SubTree：目标对的最低共同祖先下的子树，为SPTree中的路径和单词对提供了额外的修饰信息。（两种节点类型：在和不在最短依存路径上）</p></li><li><p>FullTree：完整的依赖树，捕获了整个句子的上下文。（两种节点类型：在和不在最短依存路径上）</p></li></ul><blockquote><p> 具体如何对节点进行分类，以及从上到下的树结构LSTM的节点的子节点集如何计算，我并没有找到答案。</p></blockquote><h3 id="关系分类"><a href="#关系分类" class="headerlink" title="关系分类"></a>关系分类</h3><p>在双向树结构LSTM的顶部连接一个两层的神经网络进行关系分类。用类型和方向来表示关系标签，当发现检测到的实体是错误的或这一对实体没有关系的时候，则将这一对实体视为负关系</p><p><img src="/imgs/2020071101/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96.png" srcset="/img/loading.gif" alt="关系抽取"></p><p>问题：</p><ul><li>序列层对NN的输入的贡献是间接的</li><li>只使用了实体的最后一个单词，没有充分利用实体信息</li></ul><p>解决：直接拼接两个实体所有单词在序列层的隐向量输出的平均值到d<sub>p</sub>后面</p><p><img src="/imgs/2020071101/%E6%8B%BC%E6%8E%A5dp.png" srcset="/img/loading.gif" alt="拼接dp"></p><p>此外，由于同时考虑了从左到右和从右到左的两个方向，所以在预测时为每个词对分配了两个标签。当预测的标签不一致时，选择积极和更有信心的标签。</p><h2 id="训练增强："><a href="#训练增强：" class="headerlink" title="训练增强："></a>训练增强：</h2><ul><li><p>计划抽样：在训练过程中以一定的概率ε用合法的黄金标签代替(不可靠的)预测标签</p><p>​    ε取决于epoch i，ε=k/(k+exp(i/k))，k≥1是超参，用于调节ε大小</p></li><li><p>实体预训练：使用训练数据预训练实体检测部分的模型</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>联合抽取</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths论文笔记</title>
    <link href="/2020/07/10/SDPLSTM/"/>
    <url>/2020/07/10/SDPLSTM/</url>
    
    <content type="html"><![CDATA[<h4 id="引言："><a href="#引言：" class="headerlink" title="引言："></a>引言：</h4><ul><li>SDP能够消除一句话中无用的单词，<strong>保留最有用的信息</strong>，它完全能够确定两个实体间的关系。在SDP上面的单词是对原始句子的简略表示，它包含了大量目标关系的信息。</li><li>方向很重要。依赖树是一种有向树。并且实体间的关系是有向的，r(a,b)与r(b,a)是不一样的。因此，作者认为让神经网络有能力处理这种有向性式非常必要的。出于这种考虑，作者将SDP按照公共子节点分成两个子路径，设计了两个RNN来传播从实体到公共节点的信息，捕捉关系的<strong>有向性</strong>。将沿着这两条路径提取出来的特征进行拼接之后，再进行分类。</li><li>自然语言的歧义性会提高这个任务的难度，因此<strong>整合异构语言知识</strong>对任务是有益的。为了获取异构信息，作者设计了一种<strong>多通道</strong>的循环神经网络，它充分利用了各种各样的信息，包括单词、命名实体识别的tags，上位词以及语法关系。</li><li>采用了定制的<strong>dropout策略</strong>，能够防止神经网络过拟合。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/imgs/2020071003/%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" alt="1594375994513"></p><ol><li>将每个句子使用斯坦福解析器进行解析</li><li>沿着SDP把四种不同的信息输入四个通道。每种信息都使用实数向量embedding来表示。每个通道中的信息经过LSTM后再进行maxpooling</li><li>把每个通道两个路径的池化层输出进行拼接，再把各个通道的输出进行拼接</li><li>拼接后的向量输入一个全连接隐藏层</li><li>最后连接一个softmax输出层进行分类</li></ol><h3 id="四个通道"><a href="#四个通道" class="headerlink" title="四个通道"></a>四个通道</h3><p>word embedding是最重要的，加上其它3中特征能够少量提高准确率</p><ul><li>word embedding：在大型语料库上进行无监督训练，可表示词语的语法和语义信息（word2vec）</li><li>pos embedding：word embedding所包含的信息可能与在具体的句子中的词性有关，所以将每个输入的单词和它的词性标签联合起来。本文只使用了一个粗粒度的POS类别，包含15个不同的标记（随机初始化）</li><li>dep embedding：同一对单词之间可能具有不同的依赖关系类型。本文把语法关系分为19个基于粗粒度的分类（随机初始化）</li><li>wordnet embedding：每个词都有一个更抽象的概念，上下位关系有助于在不同但概念相似的词之间建立联系。本文使用了Ciaramita and Altun 设计的工具（随机初始化）</li></ul><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>使用以下公式构建RNN可能带来梯度消失或梯度爆炸的问题</p><p><img src="/imgs/2020071003/%E5%85%AC%E5%BC%8F0.jpg" srcset="/img/loading.gif" alt="1594378692943"></p><p>LSTM使用门解决了梯度消失和爆炸问题，本文使用的是 Zaremba and Sutskever 提出的LSTM的变种<br><img src="/imgs/2020071003/%E5%85%AC%E5%BC%8F.png" srcset="/img/loading.gif" alt="公式"></p><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>dropout是一种正则化方法，随机从网络中剔除特征单元，但是一般的dropout不能和LSTM一起工作，可能损害记忆单元的有价值记忆能力</p><p>本文尝试了三种dropout方法：</p><ul><li>Dropout embeddings</li><li>在记忆单元内部Dropout，包括i<sub>t</sub>、g<sub>t</sub>、o<sub>t</sub>、c<sub>t</sub>、h<sub>t</sub></li><li>在倒数第二层的全连接隐藏层进行Dropout</li></ul><p>实验表明Dropout LSTM单元对本文的模型是有害的，而其他两种策略可以提高性能。</p><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><p>采用交叉熵来作为损失函数，使用SGD进行优化，min-batch=10</p><p><img src="/imgs/2020071003/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" srcset="/img/loading.gif" alt="损失函数"></p><h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>作者将word-embedding的维度设置为200，POS、WordNet hyponymy和语法关系embedding的维度设置为50，设置λ为10<sup>−5</sup>。</p><p>作者在embedding上使用dropout后，准确率提升了2.16%，在倒数第二层上使用dropout后，准确率提升0.16%。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>最终的F1值达到了83.7%</p><blockquote><p>参考链接：<a href="https://blog.csdn.net/dalangzhonghangxing/article/details/80240568" target="_blank" rel="noopener">https://blog.csdn.net/dalangzhonghangxing/article/details/80240568</a></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Relation classification via convolutional deep neural network论文笔记</title>
    <link href="/2020/07/10/CNNRE/"/>
    <url>/2020/07/10/CNNRE/</url>
    
    <content type="html"><![CDATA[<p><img src="/imgs/2020071002/%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" alt="模型图"></p><h2 id="一、Word-Representation部分"><a href="#一、Word-Representation部分" class="headerlink" title="一、Word Representation部分"></a>一、Word Representation部分</h2><p>有预训练的词向量比随机初始化效果要好很多，可以直接使用Turian给出的词向量</p><h2 id="二、Feature-Extraction部分"><a href="#二、Feature-Extraction部分" class="headerlink" title="二、Feature Extraction部分"></a>二、Feature Extraction部分</h2><h3 id="1-1-Lexical-level-features："><a href="#1-1-Lexical-level-features：" class="headerlink" title="1.1 Lexical level features："></a>1.1 Lexical level features：</h3><p>选择实体词和上下文词的word embedding，还有实体词的上位词的embedding，连接到词汇级特征向量L中</p><p><img src="/imgs/2020071002/%E8%AF%8D%E7%BA%A7%E7%89%B9%E5%BE%81.png" srcset="/img/loading.gif" alt="1594357454564"></p><h2 id="1-2-Sentence-Level-Features"><a href="#1-2-Sentence-Level-Features" class="headerlink" title="1.2 Sentence Level Features"></a>1.2 Sentence Level Features</h2><p>词向量不能捕捉远距离特征和语义构成，而卷积的窗口只能结合局部特征，因此提出一个包含max-pooling的卷积神经网络来提供句子级表示</p><p>每个单词进一步表示为单词特征(WF)和位置特征(PF)，结合WF和PF这个词被表示为W<sub>a</sub> = [WF,PF]<sup>T</sup>，用每个单词的词向量（WF）和位置向量（PF）经过卷积层和非线性变换提取句子层次向量</p><ul><li>若结合上下文窗口，窗口大小为3，每个单词的WF为[x<sub>n-1</sub>, x<sub>n</sub>, x<sub>n-1</sub>]</li></ul><ul><li>PF把单词相对于两个实体的相对距离表示为de维的向量，每个单词的PF为[d<sub>1</sub>,d<sub>2</sub>]</li></ul><p><strong>前向步骤：</strong> </p><ol><li>首先使用线性变换处理窗口的输出 Z = W<sub>1</sub>X ，X ∈ R<sup>n<sub>0</sub>×t</sup>，n<sub>0</sub>是向量长度，t是句子长度，W1∈R<sup>n<sub>1</sub>×n<sub>0</sub></sup>，可使用多个不同大小的卷积核，</li><li>m<sub>i</sub>=max Z(i, · )，0≤i≤n1，在Z上执行一个时间轴上的最大操作，每个隐藏层单元用最大的那一维来表示，为以下步骤提供最有用的n-gram，m的长度与句子长度无关了</li><li>g=tanh(W<sub>2</sub>m)，tanh的梯度容易计算，非线性能表示更复杂的特征，W2∈R<sup>n<sub>2</sub>×n<sub>1</sub></sup>，g能表示更高级（句子）的特征了<img src="/imgs/2020071002/%E5%8F%A5%E7%BA%A7%E7%89%B9%E5%BE%81.png" srcset="/img/loading.gif" alt="1594358073536"></li></ol><h3 id="1-3-词法特征和句法特征结合"><a href="#1-3-词法特征和句法特征结合" class="headerlink" title="1.3 词法特征和句法特征结合"></a>1.3 词法特征和句法特征结合</h3><p>将Lexical level features和Sentence Level Features进行拼接，经过W<sub>3</sub>线性层和softmax非线性层获得输出<img src="/imgs/2020071002/softmax.png" srcset="/img/loading.gif" alt="1594359552138"></p><p>使用对数似然函数计算损失函数<img src="/imgs/2020071002/likehood.png" srcset="/img/loading.gif" alt="1594359563116"></p><p>使用SGD进行参数更新<img src="/imgs/2020071002/SGD.png" srcset="/img/loading.gif" alt="1594359573866"></p><p>训练开始时对参数N、W<sub>1</sub>、W<sub>2</sub>、W<sub>3</sub>随机初始化。</p>]]></content>
    
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Survey on Knowledge Graphs: Representation, Acquisition and Applications的知识获取章节概览及相关论文列表</title>
    <link href="/2020/07/09/2020%20KG%20survey%20KA/"/>
    <url>/2020/07/09/2020%20KG%20survey%20KA/</url>
    
    <content type="html"><![CDATA[<blockquote><p>基本上是对该章节的翻译和整理，由于英文水平有限，找不到达意的翻译我就用英文单词代替，之后会对其中的几篇论文进行阅读，精读后再返回来做修改</p></blockquote><h1 id="4-知识获取"><a href="#4-知识获取" class="headerlink" title="4. 知识获取"></a>4. 知识获取</h1><p>知识获取的目的是从非结构化文本中构造知识图谱，完成现有的知识图谱，发现和识别实体和关系。<br>构造良好和大规模的知识知识图谱对许多下游应用程序非常有用，并赋予了知识感知模型常识推理的能力，从而为人工智能铺平了道路。<br>知识获取的主要任务包括关系抽取、KGC(knowledge graph completion)以及实体识别、实体对齐等面向实体的获取任务。大多数方法分别进行KGC和关系抽取。然而，这两个任务也可以集成到一个统一的框架中。  </p><pre><code>• X. Han, Z. Liu, and M. Sun, “Neural knowledge acquisition via mutual attention between  knowledge graph and text,” in AAAI, 2018, pp. 4832–4839.：联合学习框架/相互注意力/知识图和文本的数据融合[1]</code></pre><p>还有其他与知识获取相关的任务，如三重分类、关系分类等。   </p><h3 id="本章分为三个部分KGC、实体识别、关系抽取"><a href="#本章分为三个部分KGC、实体识别、关系抽取" class="headerlink" title="本章分为三个部分KGC、实体识别、关系抽取"></a>本章分为三个部分KGC、实体识别、关系抽取</h3><h1 id="4-1-KGC"><a href="#4-1-KGC" class="headerlink" title="4.1 KGC"></a>4.1 KGC</h1><p>complete现有实体之间缺失的链接或给定实体和关系查询推断出实体集。包括链路预测、实体预测和关系预测。  </p><h3 id="初始阶段研究的内容是三元组的低维嵌入"><a href="#初始阶段研究的内容是三元组的低维嵌入" class="headerlink" title="初始阶段研究的内容是三元组的低维嵌入"></a>初始阶段研究的内容是三元组的低维嵌入</h3><h3 id="4-1-1-基于嵌入的方法："><a href="#4-1-1-基于嵌入的方法：" class="headerlink" title="4.1.1 基于嵌入的方法："></a>4.1.1 基于嵌入的方法：</h3><p>缺点：无法捕捉multi-step的关系，忽视知识图的符号化性质，缺乏可解释性，因而仍停留在个体关系层面，在复杂推理方面表现较差<br>基于已存在的三元组学习出嵌入向量，然后用E中的所有实体替换头实体或尾实体来计算候选实体对的得分，并排名出前k个实体对</p><h3 id="最近研究内容转向对multi-step关系路径和incorporate（包含）逻辑规则的探索"><a href="#最近研究内容转向对multi-step关系路径和incorporate（包含）逻辑规则的探索" class="headerlink" title="最近研究内容转向对multi-step关系路径和incorporate（包含）逻辑规则的探索"></a>最近研究内容转向对multi-step关系路径和incorporate（包含）逻辑规则的探索</h3><h3 id="4-1-2-基于规则的推理"><a href="#4-1-2-基于规则的推理" class="headerlink" title="4.1.2 基于规则的推理"></a>4.1.2 基于规则的推理</h3><p>符号与嵌入的混合方法引入了基于规则的推理，克服了知识图的稀疏性，提高了嵌入的质量，方便了规则的有效注入，并归纳出了可解释规则。</p><h3 id="4-1-3-关系路径推理"><a href="#4-1-3-关系路径推理" class="headerlink" title="4.1.3 关系路径推理"></a>4.1.3 关系路径推理</h3><p>通过观察知识图的图形性质，研究了路径搜索和神经路径表示学习，但它们在遍历大规模图时存在连通性不足的问题。</p><h3 id="4-1-4-基于强化学习的路经查找"><a href="#4-1-4-基于强化学习的路经查找" class="headerlink" title="4.1.4 基于强化学习的路经查找"></a>4.1.4 基于强化学习的路经查找</h3><h3 id="4-1-5-元关系查找"><a href="#4-1-5-元关系查找" class="headerlink" title="4.1.5 元关系查找"></a>4.1.5 元关系查找</h3><p>元关系学习的新兴方向是学习在低资源环境中快速适应不可见的关系。</p><h3 id="4-1-6-三元组分类"><a href="#4-1-6-三元组分类" class="headerlink" title="4.1.6 三元组分类"></a>4.1.6 三元组分类</h3><p>KGC的相关任务，评估真实的三元组的正确性</p><h1 id="4-2-实体发现"><a href="#4-2-实体发现" class="headerlink" title="4.2 实体发现"></a>4.2 实体发现</h1><h2 id="4-2-1-实体识别"><a href="#4-2-1-实体识别" class="headerlink" title="4.2.1 实体识别"></a>4.2.1 实体识别</h2><p><strong><em>在文本中标记实体</em></strong><br>•人工特征<br>•使用端到端的神经网络来学习特征</p><pre><code>• J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional LSTM-CNNs,” Transactions of ACL, vol. 4, pp. 357–370, 2016.学习字符级和单词级特征，编码部分词汇匹配  • G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural architectures for named entity recognition,” in NAACL, 2016, pp. 260–270.通过LSTM层和CRF层的叠加，提出了多层神经结构  • C. Xia, C. Zhang, T. Yang, Y. Li, N. Du, X. Wu, W. Fan, F. Ma, and P. Yu, “Multi-grained named entity recognition,” in ACL, 2019, pp.1430–1440.针对嵌套和非重叠命名实体，集成了不同粒度的实体位置检测和基于注意的实体分类框架  </code></pre><h2 id="4-2-2-实体分类"><a href="#4-2-2-实体分类" class="headerlink" title="4.2.2 实体分类"></a>4.2.2 实体分类</h2><p>以序列到序列的方式对实体识别进行了探索，实体类型分类讨论了有噪声的类型标签和零击类型<br>实体类型包括粗粒度类型和细粒度类型，而后者使用树结构型类别，通常被视为多类和多标签分类。</p><pre><code>• X. Ren, W. He, M. Qu, C. R. Voss, H. Ji, and J. Han, “Label noise reduction in entity typing by heterogeneous partial-label embedding,” in SIGKDD, 2016, pp. 1825–1834.集中于正确的类型识别，提出了一种异构图的局部标签嵌入模型，用于表示实体提及词、文本特征和实体类型及其关系。• Y. Ma, E. Cambria, and S. Gao, “Label embedding for zero-shot fine-grained named entity typing,” in COLING, 2016, pp. 171–180 为了解决类型集的不断增长和标签的噪声，提出包含分层信息的原型驱动标签，用于zero-shot细粒度命名实体类型识别</code></pre><h2 id="4-2-3-实体消歧"><a href="#4-2-3-实体消歧" class="headerlink" title="4.2.3 实体消歧"></a>4.2.3 实体消歧</h2><p><strong><em>实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。</em></strong><br>目前流行的端到端学习方法都是通过实体和提及词的表示学习来实现的</p><pre><code>• H. Huang, L. Heck, and H. Ji, “Leveraging deep neural networks and knowledge graphs for entity  disambiguation,” arXiv preprint arXiv:1504.07678, 2015.对实体语义相似度建模• W. Fang, J. Zhang, D. Wang, Z. Chen, and M. Li, “Entity disambiguation by knowledge and text jointly embedding,” in SIGNLL, 2016, pp. 260–269.对实体和文本的联合embedding建模• O.-E. Ganea and T. Hofmann, “Deep joint entity disambiguation with local neural attention,” in EMNLP, 2017, pp. 2619–2629.提出了一种基于局部上下文窗口的注意力神经模型，用于学习实体embedding和“用于推理模糊实体”的可分辨消息传递• P. Le and I. Titov, “Improving entity linking by modeling latent relations between mentions,” in ACL, vol. 1, 2018, pp. 1595–1604.将实体之间的关系视为潜在变量，提出了一种在关系层和提及词层进行归一化的端到端神经网络。</code></pre><h2 id="4-2-4-实体对齐"><a href="#4-2-4-实体对齐" class="headerlink" title="4.2.4 实体对齐"></a>4.2.4 实体对齐</h2><p><strong><em>在异构知识图中融合知识</em></strong><br>给定两个不同知识图谱的两个实体集，实体对齐的任务是找到A = {(e 1 ,e 2 ) ∈ E 1 × E 2 |e 1 ≡ e 2 }，≡表示实体间的相等关系。在实际操作中，会给定一小组对齐种子来启动对齐过程。<br>如果新对齐的实体性能不佳，则可能会面临错误累积问题  </p><h3 id="基于embedding的对齐：计算一对实体的embedding之间的相似性。"><a href="#基于embedding的对齐：计算一对实体的embedding之间的相似性。" class="headerlink" title="基于embedding的对齐：计算一对实体的embedding之间的相似性。"></a>基于embedding的对齐：计算一对实体的embedding之间的相似性。</h3><pre><code>• H. Zhu, R. Xie, Z. Liu, and M. Sun, “Iterative entity alignment via joint knowledge embeddings,” in IJCAI, 2017, pp. 4258–4264.基于一个联合embedding框架，通过对齐转换、线性转换和参数共享把实体映射到一个表示空间• Z. Sun, W. Hu, Q. Zhang, and Y. Qu, “Bootstrapping entity alignment with knowledge graph embedding.” in IJCAI, 2018,pp. 4396–4402.为了解决迭代对齐中的误差积累问题，BootEA[100]提出了一种增量训练方式的bootstrapping方法，以及一种检查新标记对齐的编辑技术。  </code></pre><h3 id="其他研究合并了实体的其他信息以进行细化，近年来，特定语言知识的增加，必然推动了跨语言知识整合的研究。"><a href="#其他研究合并了实体的其他信息以进行细化，近年来，特定语言知识的增加，必然推动了跨语言知识整合的研究。" class="headerlink" title="其他研究合并了实体的其他信息以进行细化，近年来，特定语言知识的增加，必然推动了跨语言知识整合的研究。"></a>其他研究合并了实体的其他信息以进行细化，近年来，特定语言知识的增加，必然推动了跨语言知识整合的研究。</h3><pre><code>• Z. Sun, W. Hu, and C. Li, “Cross-lingual entity alignment via joint attribute-preserving embedding,” in ISWC, 2017, pp. 628–644.捕获跨语言属性之间的相关性• M. Chen, Y. Tian, K.-W. Chang, S. Skiena, and C. Zaniolo, “Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment,” in IJCAI, 2018, pp. 3998–4004.通过联合训练嵌入多语言实体描述• Q. Zhang, Z. Sun, W. Hu, M. Chen, L. Guo, and Y. Qu, “Multi-view knowledge graph embedding for entity alignment,” in IJCAI, 2019,pp. 5429–5435.学习实体名称、关系和属性的多个视图• B. D. Trsedya, J. Qi, and R. Zhang, “Entity alignment between knowledge graphs using attribute embeddings,” in AAAI, vol. 33,2019, pp. 297–304.使用字符属性embedding对齐</code></pre><h1 id="4-3-关系抽取"><a href="#4-3-关系抽取" class="headerlink" title="4.3 关系抽取"></a>4.3 关系抽取</h1><p><strong><em>从纯文本中提取未知关系事实，并将其添加到知识图谱中，是自动构建大规模知识图谱的关键任务</em></strong>  </p><p>由于缺乏带标签的关系数据，远程监督在关系数据库的监督下，采用启发式匹配的方法，假设包含相同实体成分的句子可能表达相同的关系来创建训练数据。在远程监督的假设下，关系提取存在噪声，特别是在不同领域的文本语料库中。因此，弱监督关系提取对于减轻噪声标注的影响非常重要</p><h2 id="传统方法高度依赖特征工程"><a href="#传统方法高度依赖特征工程" class="headerlink" title="传统方法高度依赖特征工程"></a>传统方法高度依赖特征工程</h2><pre><code>• M. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in ACL and IJCNLP of the AFNLP, 2009, pp. 1003–1011.Mintz等[106]采用远程监督的方法对关系进行分类，其文本特征包括词汇和句法特征、命名实体标签和连接词特征</code></pre><h2 id="最近的一种方法是探索特征之间的内在关联"><a href="#最近的一种方法是探索特征之间的内在关联" class="headerlink" title="最近的一种方法是探索特征之间的内在关联"></a>最近的一种方法是探索特征之间的内在关联</h2><h3 id="4-3-1-神经关系提取"><a href="#4-3-1-神经关系提取" class="headerlink" title="4.3.1 神经关系提取"></a>4.3.1 神经关系提取</h3><h4 id="使用到实体的相对位置特征的CNN网络被首先应用于关系分类"><a href="#使用到实体的相对位置特征的CNN网络被首先应用于关系分类" class="headerlink" title="使用到实体的相对位置特征的CNN网络被首先应用于关系分类"></a>使用到实体的相对位置特征的CNN网络被首先应用于关系分类</h4><pre><code>• D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classification via convolutional deep neural network,” in COLING, 2014,  pp. 2335–2344.• T. H. Nguyen and R. Grishman, “Relation extraction: Perspective  from convolutional neural networks,” in ACL Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 39–48.扩展到使用多尺度卷积核的多窗口CNN用于关系抽取</code></pre><blockquote><p><a href="https://yimial.github.io/2020/07/10/CNNRE/">Zeng</a><br><a href="https://www.cnblogs.com/sandwichnlp/p/12020066.html#model-2-relation-extraction-perspective-from-convolutional-neural-networks" target="_blank" rel="noopener">Nguyen</a></p></blockquote><h4 id="多实例学习以一个句子包作为输入，预测实体对之间的关系"><a href="#多实例学习以一个句子包作为输入，预测实体对之间的关系" class="headerlink" title="多实例学习以一个句子包作为输入，预测实体对之间的关系"></a>多实例学习以一个句子包作为输入，预测实体对之间的关系</h4><pre><code>• D. Zeng, K. Liu, Y. Chen, and J. Zhao, “Distant supervision for relation extraction via piecewise convolutional neural networks,”in EMNLP, 2015, pp. 1753–1762.与传统CNN[Zeng]相比，PCNN能够更有效地捕获实体对内部的结构信息。根据实体位置分段的最大池化• X. Jiang, Q. Wang, P. Li, and B. Wang, “Relation extraction with multi-instance multi-label convolutional neural networks,” in COLING, 2016, pp. 1471–1480.MIMLCNN进一步将其扩展到多标签学习，利用跨句最大池化进行特征选择。• H. Ye, W. Chao, Z. Luo, and Z. Li, “Jointly extracting relations with class ties via effective deep ranking,” in ACL, vol. 1, 2017, pp.1810–1820.利用class ties进行• W. Zeng, Y. Lin, Z. Liu, and M. Sun, “Incorporating relation paths in neural relation extraction,” in EMNLP, 2017, pp. 1768–1777.利用关系路径</code></pre><blockquote><p><a href="https://yimial.github.io/2020/07/12/PCNNMIL/">Zeng</a><br><a href="https://yimial.github.io/2020/07/13/MIMLDS/">Jiang</a></p></blockquote><h4 id="RNNs也被引入"><a href="#RNNs也被引入" class="headerlink" title="RNNs也被引入"></a>RNNs也被引入</h4><pre><code>• Y. Xu, L. Mou, G. Li, Y. Chen, H. Peng, and Z. Jin, “Classifying relations via long short term memory networks along shortest dependency paths,” in EMNLP, 2015, pp. 1785–1794.采用多通道LSTM，利用实体对之间的最短依赖路径• M. Miwa and M. Bansal, “End-to-end relation extraction using lstms on sequences and tree structures,” in ACL, vol. 1, 2016, pp.1105–1116.Miwa等基于依赖树的序列和树结构LSTM• R. Cai, X. Zhang, and H. Wang, “Bidirectional recurrent convolutional neural network for relation classification,” in ACL, vol. 1,2016, pp. 756–765.BRCNN利用双通道双向LSTM和CNN，将捕获序列依赖关系的RNN和表示局部语义的CNN结合起来</code></pre><blockquote><p><a href="https://yimial.github.io/2020/07/10/SDPLSTM/">Xu</a><br><a href="https://yimial.github.io/2020/07/10/SPtree/">Miwa</a><br><a href="https://zhuanlan.zhihu.com/p/22683996" target="_blank" rel="noopener">Cai</a></p></blockquote><h3 id="4-3-2-注意力机制"><a href="#4-3-2-注意力机制" class="headerlink" title="4.3.2 注意力机制"></a>4.3.2 注意力机制</h3><p>各种各样的注意力机制都可以和CNN进行结合</p><pre><code>• Y. Shen and X. Huang, “Attention-based convolutional neural network for semantic relation extraction,” in COLING, 2016, pp.2526–2536.词级别的注意力捕捉词的语义信息• Y. Lin, S. Shen, Z. Liu, H. Luan, and M. Sun, “Neural relation extraction with selective attention over instances,” in ACL, vol. 1,2016, pp. 2124–2133.对多个实例的选择性注意力，以减轻噪声实例的影响• G. Ji, K. Liu, S. He, and J. Zhao, “Distant supervision for relation extraction with sentence-level attention and entity descriptions,”in AAAI, 2017, pp. 3060–3066.APCNN引入了PCNN的实体描述和句子级注意• X. Han, P. Yu, Z. Liu, M. Sun, and P. Li, “Hierarchical relation extraction with coarse-to-fine grained attention,” in EMNLP, 2018,pp. 2236–2245.HATT提出分层选择性注意力，通过连接每个层级的注意力表示来捕获关系的层次• P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu, “Attention-based bidirectional long short-term memory networks for relation classification,” in ACL, vol. 2, 2016, pp. 207–212.Att-BLSTM没有使用基于CNN的句子编码器，而是使用BiLSTM提出了词级注意力</code></pre><h3 id="4-3-3-图卷积网络"><a href="#4-3-3-图卷积网络" class="headerlink" title="4.3.3 图卷积网络"></a>4.3.3 图卷积网络</h3><p><strong><em>GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码</em></strong>  </p><pre><code>• Y. Zhang, P. Qi, and C. D. Manning, “Graph convolution over pruned dependency trees improves relation extraction,” in EMNLP, 2018, pp. 2205–2215.基于path-centric剪枝后的句子依赖树的上下文GCN模型• Z. Guo, Y. Zhang, and W. Lu, “Attention guided graph convolutional networks for relation extraction,” in ACL, 2019, pp. 241–251.AGGCN也在依赖树上应用GCN，但采用软加权方式的多头注意力方式进行边的选择• N. Zhang, S. Deng, Z. Sun, G. Wang, X. Chen, W. Zhang, and H. Chen, “Long-tail relation extraction via knowledge graph embeddings and graph convolution networks,” in NAACL, 2019,pp. 3016–3025.将GCN用于知识图谱中关系的embedding，从而实现基于语句的关系抽取。</code></pre><h3 id="4-3-4-对抗训练"><a href="#4-3-4-对抗训练" class="headerlink" title="4.3.4 对抗训练"></a>4.3.4 对抗训练</h3><p><strong><em>在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声</em></strong>  </p><pre><code>• Y. Wu, D. Bamman, and S. Russell, “Adversarial training for relation extraction,” in EMNLP, 2017, pp. 1778–1783.• P. Qin, X. Weiran, and W. Y. Wang, “DSGAN: Generative adversarial training for distant supervision relation extraction,” in ACL,vol. 1, 2018, pp. 496–505.DSGAN通过学习一个语句级true position的生成器和一个使生成器的true position概率最小化的识别器来去除远程监督关系抽取中的噪声</code></pre><h3 id="4-3-5-强化学习"><a href="#4-3-5-强化学习" class="headerlink" title="4.3.5 强化学习"></a>4.3.5 强化学习</h3><p>近年来，通过用策略网络训练实例选择器的方式将强化学习集成到神经关系抽取中。基于强化学习的NRE的优点是关系抽取是 model-agnostic的。因此，它可以很容易地适应于任何神经结构，以有效地提取关系。</p><pre><code>• P. Qin, W. Xu, and W. Y. Wang, “Robust distant supervision relation extraction via deep reinforcement learning,” in ACL, vol. 1,2018, pp. 2137–2147 提出训练基于策略的句子关系分类器的RL代理将假阳性实例重新分布到负样本中，以减轻噪声数据的影响。以F1得分为评价指标，采用基于F1得分的性能变化作为策略网络的奖励• X. Zeng, S. He, K. Liu, and J. Zhao, “Large scaled relation  extraction with reinforcement learning,” in AAAI, 2018, pp. 5658–5665.和J. Feng, M. Huang, L. Zhao, Y. Yang, and X. Zhu, “Reinforcement  learning for relation classification from noisy data,” in AAAI, 2018,  pp. 5779–5786.提出了其他的奖励策略，• R. Takanobu, T. Zhang, J. Liu, and M. Huang, “A hierarchical  framework for relation extraction with reinforcement learning,”  in AAAI, vol. 33, 2019, pp. 7072–707 提出了一个高级关系检测和低级实体提取的层次策略学习框架</code></pre><h3 id="4-3-6-其他进展"><a href="#4-3-6-其他进展" class="headerlink" title="4.3.6 其他进展"></a>4.3.6 其他进展</h3><pre><code>• Y. Huang and W. Y. Wang, “Deep residual learning for weakly-  supervised relation extraction,” in EMNLP, 2017, pp. 1803–1807.将深度残差学习应用到噪声关系提取中，发现9层cnn的性能得到了提高。• T. Liu, X. Zhang, W. Zhou, and W. Jia, “Neural relation extraction  via inner-sentence noise reduction and transfer learning,” in EMNLP, 2018, pp. 2195–2204.提出通过实体分类中的迁移学习来初始化神经模型• K. Lei, D. Chen, Y. Li, N. Du, M. Yang, W. Fan, and Y. Shen, “Co-  operative denoising for distantly supervised relation extraction,”  in COLING, 2018, pp. 426–436.通过双向知识蒸馏和自适应模拟，将文本语料库和知识图谱与外部逻辑规则集成在一起• H. Jiang, L. Cui, Z. Xu, D. Yang, J. Chen, C. Li, J. Liu, J. Liang, C. Wang, Y. Xiao, and W. Wang, “Relation extraction using  supervision from topic knowledge of relation labels,” in IJCAI, 2019, pp. 5024–5030.通过匹配句子和主题词，丰富句子表征学习• T. Gao, X. Han, Z. Liu, and M. Sun, “Hybrid attention-based  prototypical networks for noisy few-shot relation classification,”  in AAAI, vol. 33, 2019, pp. 6407–6414.提出了基于混合注意力的原型网络来计算原型关系embedding，并比较其和query的embedding之间的距离。</code></pre><p><img src="/imgs/2020071001.png" srcset="/img/loading.gif" alt="关系抽取方法综述"></p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
      <tag>关系抽取</tag>
      
      <tag>实体识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Lambda 架构</title>
    <link href="/2020/07/07/Lambda/"/>
    <url>/2020/07/07/Lambda/</url>
    
    <content type="html"><![CDATA[<html><head></head><body><h3 id="什么是Lambda架构？">什么是Lambda架构？<a class="post-anchor" href="#什么是Lambda架构？"></a></h3><p>Nathan Marz根据他在Backtype和Twitter从事分布式数据处理系统工作的经验，提出了Lambda架构(LA)这个术语，用于通用的、可伸缩的、容错的数据处理架构。<br>LA旨在满足对具有容错性(包括硬件故障和人为错误)的健壮系统的需求，它能够广泛服务于各种工作负载和用例，并且满足了低延迟读取和更新的需求。该系统应该是线性可伸缩的，并且应该向外扩展（横向扩展）而不是向上扩展（纵向扩展）。</p><p><a href="http://lambda-architecture.net/img/la-overview_small.png" data-caption="Lambda架构" data-fancybox="images"><img src="http://lambda-architecture.net/img/la-overview_small.png" alt="Lambda架构"></a></p><ol><li>所有进入系统的数据都被发送到batch layer和speed layer进行处理。</li><li>batch layer有两个功能:(i)管理主数据集(一个不可变的、只追加的原始数据集)，(ii)预计算批处理视图。</li><li>serving layer对批处理视图进行索引，以便以低延迟、特别的方式查询它们。</li><li>speed layer补偿了serving layer更新的高延迟问题，并且只处理最近的数据。</li><li>任何传入的查询都可以通过合并来自批处理视图和实时视图的结果来回答。</li></ol><h3 id="组件">组件<a class="post-anchor" href="#组件"></a></h3><p>批处理组件：处理框架  </p><p>服务组件：合并/低延迟数据库  </p><p>速度组件：流处理框架、基于云计算的offerings、更多资源（流处理、机器学习、Storm）</p></body></html>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Questioning the Lambda Architecture阅读笔记（翻译）</title>
    <link href="/2019/11/12/Questioning%20the%20Lambda%20Architecture/"/>
    <url>/2019/11/12/Questioning%20the%20Lambda%20Architecture/</url>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>Lambda架构有其优点，但值得探索其他替代方案。</strong><br>作者先介绍了一下Lambda系统，它的优点、缺点，以及替换方案</p><h3 id="一、引言">一、引言<a class="post-anchor" href="#一、引言"></a></h3><p>Lambda架构是在MapReduce和Storm或类似系统上构建流处理应用程序的一种方法。<br><a href="https://dmgpayxepw99m.cloudfront.net/lambda-16338c9225c8e6b0c33a3f953133a4cb.png" data-caption="The Lambda Architecture" data-fancybox="images"><img src="https://dmgpayxepw99m.cloudfront.net/lambda-16338c9225c8e6b0c33a3f953133a4cb.png" alt="The Lambda Architecture"></a></p><p>其工作方式是捕获<strong>不可变</strong>的记录序列并并行地输入批处理系统和流处理系统。需要两次实现转换逻辑，一次在批处理系统中，一次在流处理系统中。在查询时将来自两个系统的结果<strong>拼接</strong>在一起，以生成完整的答案。</p><p><em>Lambda系统：一个记录序列同时输入两个系统，批处理系统隔一段时间统一预计算一次，实时系统及时进行增量计算，当某一段记录在批处理统一计算时经过了批处理系统，即可在实时系统中删除这些记录的结果。查询到来时，将批处理视图和实时视图直接拼接合并就是最新的数据。<br>批处理视图存在HDFS中，实时视图存在HBase中，拼接后使用可以在这两个存储系统上查询的Impala进行查询</em></p><p>这其中还可以有一些变化，比如换掉Storm、Kafka和Hadoop。人们通常使用两个不同的数据库来存储输出表，一个用于优化实时处理，另一个用于优化批量更新。</p><p>Lambda架构可用于处理那些围绕复杂的异步转换构建的应用程序，这些转换需要以低延迟运行(例如，几秒钟到几小时)。比如新闻推荐系统，它需要抓取各种新闻来源，处理和标准化所有的输入，然后建立索引、排序，并将其存储以供服务。</p><p>Jay Kreps认为这种方法并不是处理实时数据的最好的方式，并列出了一些优缺点</p><h3 id="二、Lambda的优点">二、Lambda的优点<a class="post-anchor" href="#二、Lambda的优点"></a></h3><p><strong>输入数据的不可变性</strong>：把data transformation看成从输入数据出发的一系列实例化阶段是非常好的。这是使大型MapReduce工作流易于处理的原因之一，因为它使您能够独立地调试每个阶段。Jay Kreps写了一些关于捕获和转换不可变数据流的<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="noopener">想法</a>。</p><p><strong>数据reprocessing</strong>：数据<strong>reprocessing</strong>是流处理的关键挑战之一，但常常被忽略。代码总是会改变的。因此，如果您有从输入流派生输出数据的代码，每当代码更改时，您都需要重新计算输出以查看更改的效果。有很多原因会造成修改代码，不论为什么要修改代码，你都需要重新生成你的输出，我发现，许多构建实时数据处理系统的人并没有在这个问题上花太多心思，最终导致系统无法快速发展，因为没有方便的方法来处理reprocessing。Lambda体系结构强调了这个问题，值得赞扬。</p><p>Lambda架构的提出还有一些其他动机，但是我认为它们没有多大意义。一个是实时处理本质上是近似计算，比批处理更弱，误差更大。我认为不是这样。确实，现有的流处理框架集不如MapReduce成熟，但是没有理由证明流处理系统不能提供与批处理系统一样强大的语义保证。<br><em>什么叫做强大的语义保证，是指向实时处理并不是一种近似计算吗？为什么有人说实时处理是近似计算？如何证明实时处理不是近似计算？</em></p><p>还有另一种解释是Lambda架构通过使用不同的权衡混合不同的数据系统，以某种方式“击败了CAP定理”。长话短说，虽然流处理中确实存在延迟/可用性权衡，但这是异步处理的体系结构，因此计算的结果不会立即与传入数据保持一致。遗憾的是，CAP定理仍然未被打破。<br><em>为什么异步处理的体系结构计算的结果不会立即与传入数据保持一致？我看着挺一致的啊？</em></p><h3 id="三、Lambda的缺点">三、Lambda的缺点<a class="post-anchor" href="#三、Lambda的缺点"></a></h3><p>Lambda架构的问题在于，需要保持两个复杂的分布式系统会生成相同的计算结果，这就像听上去这么痛苦。Jay Kreps认为这个问题解决不了。<br>在Storm和Hadoop这样的分布式框架中进行编程非常复杂。代码会被构建成相应框架的形式。实现Lambda架构的系统的操作复杂性似乎是每个人都一致同意的。</p><p>为什么不能改进流处理系统来处理其目标域中的所有问题？解决此问题的一种建议方法是使用一种语言或框架，该语言或框架对实时和批处理框架进行抽象。开发者使用这个高级框架来编写代码，然后它“向下编译”以进行流处理或MapReduce。Summingbird就是这样一个框架。这确实让事情好转了一点，但我不认为它解决了问题。</p><p>最后，即使可以避免对应用程序进行两次编码，运行和调试两个系统的操作负担也会非常大。任何一个新的抽象都只能提供这两个系统的交集所支持的特性。更糟糕的是，使用这种新的超级框架隔离了使Hadoop成为如此强大的丰富的生态系统的工具和语言(Hive、Pig、Crunch、Cascading、Oozie等)。</p><p>使跨数据库ORM真正透明是很困难的。和对那些非常相似的使用标准化的接口语言提供几乎相同的功能的系统进行抽象的问题相比，在几乎不稳定的分布式系统上构建的完全不同的编程范例是非常困难的<br><em>这一段没懂，什么是透明，什么是跨数据库ORM，这一段从哪里来，要到哪里去</em></p><h3 id="四、作者的工作">四、作者的工作<a class="post-anchor" href="#四、作者的工作"></a></h3><p>Jay Kreps在LinkedIn做了很多尝试，混合Hadoop架构甚至是一个特定领域的API，它实现了Hadoop层和实时处理层的透明化。但是这些方法都没有令人满意。隐藏底层框架的方法被证明是最有漏洞的。它最终还是需要深入了解Hadoop知识以及实时层的知识——除此之外还增加了一个新要求，即无论何时要调试或计算性能时，您都要非常了解如何将API转换为这些底层系统。</p><p>所以Jay Kreps的建议是，如果您对延迟不敏感，可以使用批处理框架(如MapReduce)，如果您对延迟敏感，可以使用流处理框架，但不要尝试同时进行这两种处理，除非您必须同时进行。</p><p>那么，为什么Lambda架构如此令人激动呢?Jay Kreps认为原因是人们越来越需要构建复杂的、低延迟的处理系统。他们手头有两个不能完全解决的问题:一个可伸缩的高延迟批处理系统可以处理历史数据，一个低延迟流处理系统不能reprocess结果。通过管道把这两个东西粘在一起，就建立了一个有用的解决方案。</p><p>在这个意义上，即使Lambda架构是痛苦的，它依然解决了一个通常会被忽略的重要问题。但Jay Kreps不认为这是一个新的范式或大数据的未来。它只是被现有工具的限制所驱动的一个临时状态。Jay Kreps认为有更好的选择。</p><h3 id="五、替代方案">五、替代方案<a class="post-anchor" href="#五、替代方案"></a></h3><p>作为一个设计基础设施的人，Jay Kreps认为最突出的问题是:<strong>为什么不能改进流处理系统来处理其目标领域的所有问题?</strong>为什么需要粘在另一个系统上?为什么不能同时进行实时处理和处理代码更改时的reprocessing?流处理系统已经有了并行的概念;为什么不通过增加并行度和非常快地重放历史来处理reprocessing呢?答案是，你可以这样做，我认为这实际上是一个合理的替代架构，如果你正在构建这种类型的系统。</p><p>当我与人讨论这个问题时，他们有时告诉我，流处理不适合于处理高吞吐量的历史数据。但我认为这种直觉主要基于他们所使用的系统的局限性，这些系统要么伸缩性很差，要么无法保存历史数据。这让他们有一种感觉，流处理系统本质上是计算一些短暂流的结果，然后丢弃所有底层数据的东西。但没有理由认为只能这样。</p><p>流处理中的基本抽象是数据流DAGs。DAG与传统数据仓库(la Volcano)中的底层抽象概念完全相同，也是MapReduce后续Tez中的底层抽象概念。流处理只是这个数据流模型的概况，它向最终用户公开中间结果的检查点和连续输出。<br><em>为什么说DAG和传统数据仓库底层抽象概念完全相同？这一段又是想说明什么？</em></p><p>那么，我们如何直接从流处理工作中进行reprocessing呢?Jay Kreps喜欢的方法其实非常简单:<br>1.使用Kafka或其他系统，可以保留您希望能够重新处理的、允许多用户访问的数据的完整<strong>日志</strong>。例如，如果您希望重新处理最多30天的数据，请将Kafka的保留时间设置为30天。<br>2.当您要进行reprocessing时，启动流处理job的另一个实例，该job从保留数据的开始处开始处理，但将此输出数据定向到新的输出表。<br>3.当第二个job完成时，将应用程序切换为从新表读取。<br>4.停止旧版本的job，并删除旧的输出表。<br><a href="https://dmgpayxepw99m.cloudfront.net/kappa-61d0afc292912b61ce62517fa2bd4309.png" data-caption="系统架构" data-fancybox="images"><img src="https://dmgpayxepw99m.cloudfront.net/kappa-61d0afc292912b61ce62517fa2bd4309.png" alt="系统架构"></a></p><p><em>那30天以前的数据呢？就不管了吗</em><br>与Lambda架构不同，在这种方法中，您只在处理代码更改时进行reprocessing并重新计算结果。当然，重新计算的job只是相同代码的改进版本，<strong>运行在相同的框架上，使用相同的输入数据</strong>。您会希望提高reprocessing job的并行性，以便它能够非常快地完成。</p><p>也许我们可以称它为Kappa架构，尽管它太简单以至于只值得一个希腊字母。</p><p><strong>撤回功能</strong><br>当然，您可以进一步优化它。一般情况下，您可以合并两个输出表。Jay Kreps认为有必要在短时间内同时拥有两者。这允许您通过一个将应用程序重定向到旧表的按钮来立即恢复到旧逻辑（<strong><em>撤回</em></strong>）。在特别重要的情况下(比如，您的广告目标标准)，您可以使用自动A/B测试或bandit算法来控制切换，以确保与之前的系统相比，您正在推出的任何bug修复或代码改进不会意外地降低性能。</p><p><strong>HDFS和Kafka集成</strong><br>请注意，这并不意味着您的数据不能存到HDFS;这只是意味着你不会在那里进行reprocessing。Kafka与Hadoop有很好的集成，所以将任何Kafka主题镜像到HDFS中都很容易。可以将流处理作业的输出流甚至中间流用于Hadoop中的分析工具(如Hive)，或者用作其他离线数据处理流的输入。</p><p>我们已经使用Samza实现了此方法以及其他reprocessing体系结构的变种。</p><h3 id="六、一些背景（Kafka）">六、一些背景（Kafka）<a class="post-anchor" href="#六、一些背景（Kafka）"></a></h3><p>对于那些不太熟悉Kafka的人来说，我刚才所描述的可能没有意义。快速复习一下可能会把事情弄清楚。Kafka保持这样的有序日志:<br><em>知道你不早说</em><br><a href="https://dmgpayxepw99m.cloudfront.net/log-fa00884b8ef635ea9f32dc12180edd56.png" data-caption="Kafka的日志" data-fancybox="images"><img src="https://dmgpayxepw99m.cloudfront.net/log-fa00884b8ef635ea9f32dc12180edd56.png" alt="Kafka的日志"></a>Kafka的“主题”是这些日志的集合：<br><a href="https://dmgpayxepw99m.cloudfront.net/partitioned_log-c74280b2059abf3b78d9bbddd657e0d7.png" data-caption="Kafka的日志集" data-fancybox="images"><img src="https://dmgpayxepw99m.cloudfront.net/partitioned_log-c74280b2059abf3b78d9bbddd657e0d7.png" alt="Kafka的日志集"></a></p><p>使用此数据的流处理器只维护一个“偏移量”，即它在每个分区上处理的<strong>最后一条</strong>记录的日志条目号。因此，<strong>更改使用者的位置</strong>以返回并重新处理数据非常简单，只需使用不同的偏移量重新启动作业即可。为相同的数据添加第二个使用者只是 <em>添加</em> 另一个指向日志中不同位置的读取器。<br><em>不管偏移量在哪，日志文件保持近三十天的内容不变，重启作业时，改作业顺着日志走到头。</em></p><p>Kafka支持复制和容错，运行在廉价的普通硬件上，并且很乐意在每台机器上存储很多TBs数据。因此，保留大量数据是一种非常自然和经济的做法，不会影响性能。LinkedIn在网上保留了pb级的Kafka数据，许多应用程序都很好地利用了这种长时间的保留模式。</p><p>廉价的消费和保留大量数据的能力使得添加第二个“reprocessing” job仅仅是<strong>启动代码的第二个实例</strong>，只是要从日志中的不同位置开始。<br><em>那日志没有保存的操作就不用重新执行了吗</em><br><em>刚好就很适合做流处理的reprocessing</em><br>这个设计不是偶然的。我们构建Kafka的目的就是把它作为流处理的基础来使用，而且我们想要的正是进行reprocessing的模型。好奇的读者可以在<a href="https://kafka.apache.org/documentation.html#introduction" target="_blank" rel="noopener">这里</a>找到更多关于卡夫卡的信息。</p><p>然而，从根本上说，没有任何东西把这个想法与Kafka联系起来，<em>Kafka就是个日志系统而已</em>。您可以用这个方法替换任何支持长期保留有序数据的系统（如HDFS）。实际上，很多人都熟悉类似的模式，即事件源或CQRS。当然，分布式数据库的人会告诉你，这只是实例化视图维护的一个rebranding，他们会很高兴地提醒你，他们很久以前就知道了，sonny。</p><h3 id="七、比较">七、比较<a class="post-anchor" href="#七、比较"></a></h3><p>Jay Kreps指出这种方法可以将Samza用作流处理系统，因为他们在LinkedIn上就是这么做的。但是Jay Kreps不知道<strong>它在Storm或其他流处理系统中不能同样有效</strong>的原因。“我对Storm不是很熟悉，所以我想知道其他人是否已经在这么做了”。无论如何，Jay Kreps认为总体思想是与系统无关的。<br><em>是啊为什么呢</em></p><p>这两种方法之间的效率和资源权衡有点像wash。Lambda架构要求<strong>一直运行</strong>reprocessing和实时处理，而Jay Krep所建议的只需要<strong>在需要reprocessing时</strong>运行job的第二个副本。</p><p>但是，Jay Kreps的建议要求在输出数据库中暂时拥有<strong>2倍的存储空间</strong>，并且需要一个<strong>支持大容量写入</strong>的数据库来进行重新加载。在这两种情况下，重新处理的额外负荷可能会被平均化。如果您有很多这样的工作，它们将不会一次全部reprocess，因此在具有数十个这样的工作的共享群集上，您可以为可能会在任何给定时间进行reprocessing的少数工作<strong>预留</strong>额外的百分之几的容量。</p><p>Kafka真正的优势不在于效率，而在于允许人们在单一的处理框架上开发、测试、调试和操作他们的系统。因此，<strong>在简单性很重要的情况下</strong>，可以将此方法视为Lambda架构的替代方案。</p><h2 id="启发">启发<a class="post-anchor" href="#启发"></a></h2><p>维持两个系统同状态很难，那就只维持一个系统，另一个系统给它换成这个系统。<br>为什么这个方法在Storm或其他流处理系统中不能同样有效<br>有没有什么办法改进kappa对存储空间或大容量写入的要求<br>替换方案不一定要全方位比原方法好，可以只考虑其中一个需求<br>日志系统中删除的日志如何重计算</p><p><a href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" target="_blank" rel="noopener">[1]Jay Kreps.Questioning the Lambda Architecture</a></p></body></html>]]></content>
    
    
    
    <tags>
      
      <tag>Lambda</tag>
      
      <tag>大数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Lambda architecture——principles for architecting realtime Big Data systems阅读笔记（翻译）</title>
    <link href="/2019/11/10/Lambda%20detail/"/>
    <url>/2019/11/10/Lambda%20detail/</url>
    
    <content type="html"><![CDATA[<p><strong>query = function(all data)</strong><br>本文介绍了Lambda架构的三个层的任务和特点，最后就Lambda各个设定的意义展开了思考</p><h3 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h3><p>Nathan Marz和James Warren合著的《大数据——可伸缩实时数据系统的原则和最佳实践》介绍了Lambda架构。Lambda是第一个真正将批处理和流处理协作解决无数用例的体系结构。  </p><p><strong>Lambda架构的前提：</strong>可以对所有数据运行ac-hoc查询来获得结果，但是这样做资源开销很大。  </p><p><em>ac-hoc查询：即席查询是指那些用户在使用系统时，根据自己当时的需求定义的查询。</em></p><p>从技术上讲，现在对大数据运行ac-hoc查询(Cloudera Impala)是可行的，但在每次需要计算URL的页面访问量时查询pb级数据可能不是最有效的方法。因此，我们的想法是将结果预计算为一组视图，然后查询这些视图。</p><h3 id="二、Lambda架构分为三个层次"><a href="#二、Lambda架构分为三个层次" class="headerlink" title="二、Lambda架构分为三个层次"></a>二、Lambda架构分为三个层次</h3><h4 id="1-批处理层-Apache-Hadoop"><a href="#1-批处理层-Apache-Hadoop" class="headerlink" title="1.批处理层(Apache Hadoop)"></a>1.批处理层(Apache Hadoop)</h4><p><strong>存储、计算</strong></p><p>批处理层负责两件事。第一个是<strong>存储</strong>不可变的、不断增长的主数据集(HDFS)，第二个是从这个数据集<strong>计算</strong>任意视图(MapReduce)。</p><p><strong>迭代计算</strong></p><p>计算视图是一个持续的操作，当新数据到达时，它将在下一次MapReduce迭代的重计算过程中聚合到视图中。<br>视图计算整个数据集，因此批处理层不需要频繁地<strong>更新</strong>视图。根据数据集和集群的大小，每次迭代可能需要几个小时。</p><h4 id="2-服务层-Cloudera-Impala"><a href="#2-服务层-Cloudera-Impala" class="headerlink" title="2.服务层(Cloudera Impala)"></a>2.服务层(Cloudera Impala)</h4><p><strong>索引、Impala公开视图</strong></p><p>批处理层的输出是一组包含预计算视图的平面文件。服务层负责<strong>索引</strong>和<strong>公开视图</strong>，以便查询。<br>由于批处理视图是静态的，所以服务层只需要提供批量更新和随机读取功能，为此我们将使用Cloudera Impala。要使用Impala公开视图，服务层需要做的就是在Hive Metastore中创建一个指向HDFS文件的表（索引？）。用户将能够使用Impala立即查询视图。</p><ul><li>Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。</li><li>Impala的定位是OLAP</li></ul><p><strong>引出速度层</strong></p><p>Hadoop和Impala是批处理和服务层的完美工具。Hadoop可以<strong>存储和处理</strong>pb级的数据，Impala可以快速、交互式地<strong>查询</strong>这些数据。尽管如此，批处理和服务层本身并不满足任何实时需求，因为MapReduce(按设计)是高延迟的，可能需要几个小时才能<strong>在视图中表示新数据并将其传播到服务层</strong>。这就是为什么我们需要速度层。  </p><p>请注意“实时”一词的用法。当作者说实时时，实际上作者指的是near-实时(NRT)，也就是事件发生与该事件的处理出任何数据之间的时间延迟。</p><pre><code>In the Lambda architecture, realtime is the ability to process the delta of data that has been captured after the start of the batch layers current iteration and now.</code></pre><h4 id="3-速度层"><a href="#3-速度层" class="headerlink" title="3.速度层"></a>3.速度层</h4><p><strong>增量，对应批处理层的迭代计算</strong></p><p>本质上，速度层与批处理层相同，它也使用接收到的数据计算视图。但速度层需要补偿批处理层的高延迟，它通过在Storm中计算实时视图来达到这个目的。实时视图<strong>只包含</strong>对批处理视图进行补充的增量结果。<br>批处理层从头开始不断地重新计算批处理视图，而速度层使用<strong>增量模型</strong>，当接收到新数据时，递增实时视图。</p><p><strong>临时性，批处理层处理后即删除</strong></p><p>临时性：speed层的聪明之处在于实时视图是瞬态的，一旦数据通过了批处理和服务层的处理，实时视图中的相应结果就可以丢弃。在书中，这被称为“复杂性隔离”，这意味着架构中最复杂的部分被推到结果是临时存在的层中。<br><img src="https://66.media.tumblr.com/18bde56255d7f692cf96311edba64cd1/04f815501c14a923-ac/s500x750/a6801f141738ab137db8d26188e7082a21175f86.png" srcset="/img/loading.gif" alt="临时性"></p><p><strong>Storm计算实时视图、HBase存储实时视图、Impala查询合并视图</strong></p><p>最后一个难题是公开实时视图，以便对他们进行查询并与批处理视图<strong>合并</strong>，以获得完整的结果。由于实时视图是增量的，所以speed层需要随机读写，为此我们将使用Apache HBase。HBase为Storm提供了持续increment实时视图的能力，同时可以通过Impala查询与批处理视图的合并结果。<br>Impala既可以查询存储在HDFS中的批处理视图，又可以查询存储在HBase中的实时视图，这使得它成为完成这项任务的完美工具。</p><h3 id="三、思考"><a href="#三、思考" class="headerlink" title="三、思考"></a>三、思考</h3><p>Hadoop使得我们更方便地查询和更新数据。</p><p><strong>不可变记录的意义</strong></p><p>不可变记录（record）是一个记录在某一时刻的版本。可以添加记录的较新版本，但不能覆盖特定的版本，这意味着始终可以恢复到以前的状态。在Lambda架构中，这意味着如果您不小心添加了一些坏的记录，可以通过简单地删除这些记录，然后重新计算视图以修复问题。这本书把这个称为“human fault-tolerance”</p><p><strong>recompute的意义</strong><br>您可能会认为从头计算是一个bad idea，而且实现增量MapReduce算法来增加批处理视图更新的频率肯定会更有性能。尽管这样，你依然会牺牲性能来换取human fault-tolerance，因为在使用增量算法时，要修复视图中的任何问题都要困难得多。<br><em>Storm层一般无法进行recompute，因为流处理层一般不保存数据，重现数据的增量计算很难</em></p><p><strong>仅使用Hadoop生态系统能否实现相同的结果？（整个不懂，下次再学，总而言之兴许可以，但没必要）</strong><br>作者认为实现这种体系结构的原因在于您对实时的看法和需求，以及您是否认为human fault-tolerance是系统中的一个重要因素。在Hadoop中实现低延迟系统是可行的。例如，可以使用Apache Flume创建一个通到HDFS或HBase的ingest管道，并使用Impala查询数据。Flume的最新版本(1.2.0)还引入了拦截器的概念，可用于修改流数据。但Flume by design不是Storm那样的流媒体分析平台，因此作者认为很难在Flume中计算实时视图(但也不是不可能)。另一方面，Storm是一个专门构建的、可伸缩的流处理系统，它的延迟通常要低得多。</p><p>作者从大数据这本书(至少是前6章)中学到最多的是这个<strong>架构的原理</strong>。还有<strong>不变性和human fault-tolerance的重要性</strong>，以及<strong>预计算和重计算的好处</strong>。如果您正在考虑完整地实现Lambda架构，请问自己一个问题:我需要多实时?如果你的答案是在几十秒，那么完整的Lambda架构可能没有必要，但如果你的答案是毫秒，那么作者认为Lambda架构就是你的答案。</p><p>作为这篇文章的前导，作者一直在为Storm开发一个HBase连接器。连接器为在Lambda架构中创建实时视图提供了大量的Storm Bolt和Trident state的实现。</p><h2 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h2><p>通过批处理视图迭代重计算+实时视图增量计算的方式，查询时拼接视图，使得系统架构可以更好的适用于这两种需求：满足容错性和实时性。</p><p><a href="https://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for" target="_blank" rel="noopener">[1] James Kinley.The Lambda architecture——principles for architecting realtime Big Data systems</a><br><a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html" target="_blank" rel="noopener">这篇博客的源博客的源博客：How to beat the CAP theorem</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Lambda</tag>
      
      <tag>大数据处理框架</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
