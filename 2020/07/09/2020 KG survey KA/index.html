<!DOCTYPE html>
<html>
  <head>
      <script>
  var _hmt = _hmt || []
  ;(function() {
    var hm = document.createElement('script')
    hm.src = 'https://hm.baidu.com/hm.js?5a0acc897fd96474a2c8f4deac84611a'
    var s = document.getElementsByTagName('script')[0]
    s.parentNode.insertBefore(hm, s)
  })()
</script> 
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="keywords" content="" />
    <meta name="description" content="" />
    
    <title>
      A Survey on Knowledge Graphs: Representation, Acquisition and Applications的知识获取章节概览及相关论文列表 - Hexo
    </title>
    <link rel="manifest" href="/manifest.json" />
    <link rel="shortcut icon" href="/images/favicon.jpg" type="image/x-icon" />
    
<link rel="stylesheet" href="/style/style.css">

  <meta name="generator" content="Hexo 4.2.1"></head>
  <body>
    <canvas id='pagemap'></canvas>
    
    <div id="post-toc" class="animated hiddenToc hide">
      <span class="title">Toc</span>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#4-知识获取"><span class="toc-text">4. 知识获取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-1-KGC"><span class="toc-text">4.1 KGC</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-基于嵌入的方法："><span class="toc-text">4.1.1 基于嵌入的方法：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-基于规则的推理"><span class="toc-text">4.1.2 基于规则的推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3-关系路径推理"><span class="toc-text">4.1.3 关系路径推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-4-基于强化学习的路经查找"><span class="toc-text">4.1.4 基于强化学习的路经查找</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-5-元关系查找"><span class="toc-text">4.1.5 元关系查找</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-6-三元组分类"><span class="toc-text">4.1.6 三元组分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-2-实体发现"><span class="toc-text">4.2 实体发现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-1-实体识别：在文本中标记实体"><span class="toc-text">4.2.1 实体识别：在文本中标记实体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-2-实体分类"><span class="toc-text">4.2.2 实体分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-3-实体消歧：实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。"><span class="toc-text">4.2.3 实体消歧：实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-4-实体对齐：在异构知识图中融合知识"><span class="toc-text">4.2.4 实体对齐：在异构知识图中融合知识</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-3-关系抽取"><span class="toc-text">4.3 关系抽取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-神经关系提取"><span class="toc-text">4.3.1 神经关系提取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-注意力机制"><span class="toc-text">4.3.2 注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-图卷积网络：GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码"><span class="toc-text">4.3.3 图卷积网络：GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-对抗训练：在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声"><span class="toc-text">4.3.4 对抗训练：在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5-强化学习"><span class="toc-text">4.3.5 强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-6-其他进展"><span class="toc-text">4.3.6 其他进展</span></a></li></ol></li></ol></li></ol>
    </div>
    
    <div id="fixed-menu-wrap">
      <span class="iconfont icon-sousuo search-box menu-reset"></span>
      <span class="icon-toc menu-reset">Toc</span>
      <span class="iconfont icon-arrowup menu-reset"></span>
    </div>
    <div id="fixed-menu">
      <span class="iconfont icon-menu-"></span>
    </div>
    <div id="progress">
      <div class="line"></div>
    </div>
    <div id="search-shade" class="animated hiddenSearch hide">
      <div class="input-wrap">
        <span class="iconfont icon-sousuo search-box"></span>
        <input type="text" placeholder="Search" />
        <span class="iconfont icon-close"></span>
      </div>
      <div class="search-result">
        <div class="meta">
          <span><b id="result-count">0</b> results found</span>
          <img src="/images/logo.jpeg" />
        </div>
        <ul id="result-box"></ul>
      </div>
    </div>
    <div id="menu-mask" class="animated hideMenuMask hide">
      <span class="iconfont icon-close"></span>
      <div class="nav">
        
        <a href="/" class="">
          Home
        </a>
        
        <a href="/archives" class="">
          Archives
        </a>
        
        <a href="/categories" class="">
          Categories
        </a>
        
        <a href="/tags" class="">
          Tags
        </a>
        
        <a href="/friends" class="">
          Friends
        </a>
        
        <a href="/about" class="">
          About
        </a>
        
      </div>
    </div>
    <div id="header">
      <div class="intro">
        <a href="/" class="logo" style="background-image: url('/images/logo.jpeg')"></a>
        <div class="author">执杭</div>
      </div>
      <div class="nav">
        <span class="iconfont icon-menu menu-icon"></span>
        <a href="#" class="search-box">
          <span class="iconfont icon-sousuo"></span>
        </a>
      </div>
    </div>
    <div id="side" class="animated bounceInLeft">
      <div class="shrink">
        <a href="/" class="logo" style="background-image: url('/images/logo.jpeg')"></a>
        <span class="iconfont icon-menu toggle-icon"></span>
        <a href="#" class="search-box">
          <span class="iconfont icon-sousuo"></span>
        </a>
      </div>
      <div class="magnify">
        <div class="about">
          <div class="author">执杭</div>
          <a href="/" class="logo" style="background-image: url('/images/logo.jpeg')"></a>
        </div>

        <div class="nav">
          
          <a href="/" class="">
            Home
          </a>
          
          <a href="/archives" class="">
            Archives
          </a>
          
          <a href="/categories" class="">
            Categories
          </a>
          
          <a href="/tags" class="">
            Tags
          </a>
          
          <a href="/friends" class="">
            Friends
          </a>
          
          <a href="/about" class="">
            About
          </a>
          
          <a href="#" class="search-box">
            <span class="iconfont icon-sousuo"></span>
          </a>
        </div>
        <div class="bottom">
          <div class="follow">
            
            <a href="https://github.com/shixiaohu2206" target="_block">
              <span class="iconfont icon-github"></span>
            </a>
            
            <a href="https://www.facebook.com/shi.xiaohu.5" target="_block">
              <span class="iconfont icon-facebook"></span>
            </a>
            
            <a href="https://www.instagram.com/shi_xiaohu/" target="_block">
              <span class="iconfont icon-instagram"></span>
            </a>
             
            <a href="/atom.xml" target="_block">
              <span class="iconfont icon-rss"></span>
            </a>
            
          </div>
        </div>
      </div>
    </div>
    <div id="container">
      <div class="main animated bounceInRight delay-0.7s">
        <article class="post-entry">
    <div class="header">
      
      <div class="title">A Survey on Knowledge Graphs: Representation, Acquisition and Applications的知识获取章节概览及相关论文列表</div>
      <div class="meta">
        <span class="item">
          <span class="iconfont icon-time-circle"></span>
          <span>2020/07/09</span>
        </span>

        
          <span class="item leancloud-visitors" id="/2020/07/09/2020%20KG%20survey%20KA/" data-flag-title="A Survey on Knowledge Graphs: Representation, Acquisition and Applications的知识获取章节概览及相关论文列表">
            <span class="iconfont icon-eye1"></span>
            <span class="leancloud-visitors-count"></span>
          </span>
        

        
        
         
          <span class="item">
            <span class="iconfont icon-tag1"></span>
            <span>
                
                  
                    <a href="/tags/知识图谱">知识图谱</a>
                  
                
                  
                    <a href="/tags/关系抽取">关系抽取</a>
                  
                
                  
                    <a href="/tags/实体识别">实体识别</a>
                  
                
            </span>
          </span>
         
      </div>
      <div>
      </div>
    </div>
    <html><head></head><body><blockquote>
<p>基本上是对该章节的翻译和整理，由于英文水平有限，找不到达意的翻译我就用英文单词代替，之后会对其中的几篇论文进行阅读，精读后再返回来做修改</p>
</blockquote>
<h1 id="4-知识获取">4. 知识获取<a class="post-anchor" href="#4-知识获取"></a></h1><p>知识获取的目的是从非结构化文本中构造知识图谱，完成现有的知识图谱，发现和识别实体和关系。<br>构造良好和大规模的知识知识图谱对许多下游应用程序非常有用，并赋予了知识感知模型常识推理的能力，从而为人工智能铺平了道路。<br>知识获取的主要任务包括关系抽取、KGC(knowledge graph completion)以及实体识别、实体对齐等面向实体的获取任务。大多数方法分别进行KGC和关系抽取。然而，这两个任务也可以集成到一个统一的框架中。  </p>
<p>• X. Han, Z. Liu, and M. Sun, “Neural knowledge acquisition via mutual attention between  knowledge graph and text,” in AAAI, 2018, pp. 4832–4839.：联合学习框架/相互注意力/知识图和文本的数据融合[1]<br>还有其他与知识获取相关的任务，如三重分类、关系分类等。 </p>
<p><strong>本章分为三个部分KGC、实体识别、关系抽取</strong></p>
<h1 id="4-1-KGC">4.1 KGC<a class="post-anchor" href="#4-1-KGC"></a></h1><p>complete现有实体之间缺失的链接或给定实体和关系查询推断出实体集。包括链路预测、实体预测和关系预测。  </p>
<p><strong>初始阶段研究的内容是三元组的低维嵌入</strong></p>
<h3 id="4-1-1-基于嵌入的方法：">4.1.1 基于嵌入的方法：<a class="post-anchor" href="#4-1-1-基于嵌入的方法："></a></h3><p>缺点：无法捕捉multi-step的关系，忽视知识图的符号化性质，缺乏可解释性，因而仍停留在个体关系层面，在复杂推理方面表现较差<br>基于已存在的三元组学习出嵌入向量，然后用E中的所有实体替换头实体或尾实体来计算候选实体对的得分，并排名出前k个实体对</p>
<p><strong>最近研究内容转向对multi-step关系路径和incorporate（包含）逻辑规则的探索</strong></p>
<h3 id="4-1-2-基于规则的推理">4.1.2 基于规则的推理<a class="post-anchor" href="#4-1-2-基于规则的推理"></a></h3><p>符号与嵌入的混合方法引入了基于规则的推理，克服了知识图的稀疏性，提高了嵌入的质量，方便了规则的有效注入，并归纳出了可解释规则。</p>
<h3 id="4-1-3-关系路径推理">4.1.3 关系路径推理<a class="post-anchor" href="#4-1-3-关系路径推理"></a></h3><p>通过观察知识图的图形性质，研究了路径搜索和神经路径表示学习，但它们在遍历大规模图时存在连通性不足的问题。</p>
<h3 id="4-1-4-基于强化学习的路经查找">4.1.4 基于强化学习的路经查找<a class="post-anchor" href="#4-1-4-基于强化学习的路经查找"></a></h3><h3 id="4-1-5-元关系查找">4.1.5 元关系查找<a class="post-anchor" href="#4-1-5-元关系查找"></a></h3><p>元关系学习的新兴方向是学习在低资源环境中快速适应不可见的关系。</p>
<h3 id="4-1-6-三元组分类">4.1.6 三元组分类<a class="post-anchor" href="#4-1-6-三元组分类"></a></h3><p>KGC的相关任务，评估真实的三元组的正确性</p>
<h1 id="4-2-实体发现">4.2 实体发现<a class="post-anchor" href="#4-2-实体发现"></a></h1><h2 id="4-2-1-实体识别：在文本中标记实体">4.2.1 实体识别：在文本中标记实体<a class="post-anchor" href="#4-2-1-实体识别：在文本中标记实体"></a></h2><p>人工特征<br><strong>使用端到端的神经网络来学习特征</strong></p>
<pre><code>• J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional LSTM-CNNs,” Transactions of ACL, vol. 4, pp. 357–370, 2016.学习字符级和单词级特征，编码部分词汇匹配  
<br>• G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural architectures for named entity recognition,” in NAACL, 2016, pp. 260–270.通过LSTM层和CRF层的叠加，提出了多层神经结构  
<br>• C. Xia, C. Zhang, T. Yang, Y. Li, N. Du, X. Wu, W. Fan, F. Ma, and P. Yu, “Multi-grained named entity recognition,” in ACL, 2019, pp.1430–1440.针对嵌套和非重叠命名实体，集成了不同粒度的实体位置检测和基于注意的实体分类框架  </code></pre><h2 id="4-2-2-实体分类">4.2.2 实体分类<a class="post-anchor" href="#4-2-2-实体分类"></a></h2><p>以序列到序列的方式对实体识别进行了探索，实体类型分类讨论了有噪声的类型标签和零击类型<br>实体类型包括粗粒度类型和细粒度类型，而后者使用树结构型类别，通常被视为多类和多标签分类。  </p>
<pre><code>• X. Ren, W. He, M. Qu, C. R. Voss, H. Ji, and J. Han, “Label noise reduction in entity typing by heterogeneous partial-label embedding,” in SIGKDD, 2016, pp. 1825–1834.集中于正确的类型识别，提出了一种异构图的局部标签嵌入模型，用于表示实体提及词、文本特征和实体类型及其关系。
<br>• Y. Ma, E. Cambria, and S. Gao, “Label embedding for zero-shot fine-grained named entity typing,” in COLING, 2016, pp. 171–180 为了解决类型集的不断增长和标签的噪声，提出包含分层信息的原型驱动标签，用于zero-shot细粒度命名实体类型识别</code></pre><h2 id="4-2-3-实体消歧：实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。">4.2.3 实体消歧：实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。<a class="post-anchor" href="#4-2-3-实体消歧：实体消歧或实体链接是将实体提及词链接到知识图谱中对应的实体的一项任务。"></a></h2><p>目前流行的端到端学习方法都是通过实体和提及词的表示学习来实现的</p>
<pre><code>• H. Huang, L. Heck, and H. Ji, “Leveraging deep neural networks and knowledge graphs for entity  disambiguation,” arXiv preprint arXiv:1504.07678, 2015.对实体语义相似度建模
<br>• W. Fang, J. Zhang, D. Wang, Z. Chen, and M. Li, “Entity disambiguation by knowledge and text jointly embedding,” in SIGNLL, 2016, pp. 260–269.对实体和文本的联合embedding建模
<br>• O.-E. Ganea and T. Hofmann, “Deep joint entity disambiguation with local neural attention,” in EMNLP, 2017, pp. 2619–2629.提出了一种基于局部上下文窗口的注意力神经模型，用于学习实体embedding和“用于推理模糊实体”的可分辨消息传递
<br>• P. Le and I. Titov, “Improving entity linking by modeling latent relations between mentions,” in ACL, vol. 1, 2018, pp. 1595–1604.将实体之间的关系视为潜在变量，提出了一种在关系层和提及词层进行归一化的端到端神经网络。</code></pre><h2 id="4-2-4-实体对齐：在异构知识图中融合知识">4.2.4 实体对齐：在异构知识图中融合知识<a class="post-anchor" href="#4-2-4-实体对齐：在异构知识图中融合知识"></a></h2><p>给定两个不同知识图谱的两个实体集，实体对齐的任务是找到A = {(e 1 ,e 2 ) ∈ E 1 × E 2 |e 1 ≡ e 2 }，≡表示实体间的相等关系。在实际操作中，会给定一小组对齐种子来启动对齐过程。<br>如果新对齐的实体性能不佳，则可能会面临错误累积问题<br>###基于embedding的对齐：计算一对实体的embedding之间的相似性。</p>
<pre><code>• H. Zhu, R. Xie, Z. Liu, and M. Sun, “Iterative entity alignment via joint knowledge embeddings,” in IJCAI, 2017, pp. 4258–4264.基于一个联合embedding框架，通过对齐转换、线性转换和参数共享把实体映射到一个表示空间
<br>• Z. Sun, W. Hu, Q. Zhang, and Y. Qu, “Bootstrapping entity alignment with knowledge graph embedding.” in IJCAI, 2018,pp. 4396–4402.为了解决迭代对齐中的误差积累问题，BootEA[100]提出了一种增量训练方式的bootstrapping方法，以及一种检查新标记对齐的编辑技术。</code></pre><p>###其他研究合并了实体的其他信息以进行细化，近年来，特定语言知识的增加，必然推动了跨语言知识整合的研究。</p>
<pre><code>• Z. Sun, W. Hu, and C. Li, “Cross-lingual entity alignment via joint attribute-preserving embedding,” in ISWC, 2017, pp. 628–644.捕获跨语言属性之间的相关性
<br>• M. Chen, Y. Tian, K.-W. Chang, S. Skiena, and C. Zaniolo, “Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment,” in IJCAI, 2018, pp. 3998–4004.通过联合训练嵌入多语言实体描述
<br>• Q. Zhang, Z. Sun, W. Hu, M. Chen, L. Guo, and Y. Qu, “Multi-view knowledge graph embedding for entity alignment,” in IJCAI, 2019,pp. 5429–5435.学习实体名称、关系和属性的多个视图
<br>• B. D. Trsedya, J. Qi, and R. Zhang, “Entity alignment between knowledge graphs using attribute embeddings,” in AAAI, vol. 33,2019, pp. 297–304.使用字符属性embedding对齐</code></pre><h1 id="4-3-关系抽取">4.3 关系抽取<a class="post-anchor" href="#4-3-关系抽取"></a></h1><p>从纯文本中提取未知关系事实，并将其添加到知识图谱中，是自动构建大规模知识图谱的关键任务  </p>
<p>由于缺乏带标签的关系数据，远程监督在关系数据库的监督下，采用启发式匹配的方法，假设包含相同实体成分的句子可能表达相同的关系来创建训练数据。在远程监督的假设下，关系提取存在噪声，特别是在不同领域的文本语料库中。因此，弱监督关系提取对于减轻噪声标注的影响非常重要</p>
<p>##传统方法高度依赖特征工程</p>
<pre><code>• M. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in ACL and IJCNLP of the AFNLP, 2009, pp. 1003–1011.Mintz等[106]采用远程监督的方法对关系进行分类，其文本特征包括词汇和句法特征、命名实体标签和连接词特征</code></pre><p>##最近的一种方法是探索特征之间的内在关联</p>
<h3 id="4-3-1-神经关系提取">4.3.1 神经关系提取<a class="post-anchor" href="#4-3-1-神经关系提取"></a></h3><pre><code>• D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classifica-  tion via convolutional deep neural network,” in COLING, 2014,  pp. 2335–2344.使用到实体的相对位置特征的CNN网络被首先应用于关系分类
<br>• T. H. Nguyen and R. Grishman, “Relation extraction: Perspective  from convolutional neural networks,” in ACL Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 39–48.扩展到使用多尺度卷积核的多窗口CNN用于关系抽取</code></pre><p>####多实例学习以一个句子包作为输入，预测实体对之间的关系</p>
<pre><code>• D. Zeng, K. Liu, Y. Chen, and J. Zhao, “Distant supervision for relation extraction via piecewise convolutional neural networks,”in EMNLP, 2015, pp. 1753–1762.与传统CNN[Zeng]相比，PCNN能够更有效地捕获实体对内部的结构信息。根据实体位置分段的最大池化
<br>• X. Jiang, Q. Wang, P. Li, and B. Wang, “Relation extraction with multi-instance multi-label convolutional neural networks,” in COLING, 2016, pp. 1471–1480.MIMLCNN进一步将其扩展到多标签学习，利用跨句最大池化进行特征选择。
<br>• H. Ye, W. Chao, Z. Luo, and Z. Li, “Jointly extracting relations with class ties via effective deep ranking,” in ACL, vol. 1, 2017, pp.1810–1820.利用class ties进行
<br>• W. Zeng, Y. Lin, Z. Liu, and M. Sun, “Incorporating relation paths in neural relation extraction,” in EMNLP, 2017, pp. 1768–1777.利用关系路径</code></pre><p>####RNNs也被引入</p>
<pre><code>• Y. Xu, L. Mou, G. Li, Y. Chen, H. Peng, and Z. Jin, “Classifying relations via long short term memory networks along shortest dependency paths,” in EMNLP, 2015, pp. 1785–1794.采用多通道LSTM，利用实体对之间的最短依赖路径
<br>• M. Miwa and M. Bansal, “End-to-end relation extraction using lstms on sequences and tree structures,” in ACL, vol. 1, 2016, pp.1105–1116.Miwa等基于依赖树的序列和树结构LSTM
<br>• R. Cai, X. Zhang, and H. Wang, “Bidirectional recurrent convolutional neural network for relation classification,” in ACL, vol. 1,2016, pp. 756–765.BRCNN利用双通道双向LSTM和CNN，将捕获序列依赖关系的RNN和表示局部语义的CNN结合起来</code></pre><h3 id="4-3-2-注意力机制">4.3.2 注意力机制<a class="post-anchor" href="#4-3-2-注意力机制"></a></h3><p>各种各样的注意力机制都可以和CNN进行结合</p>
<pre><code>• Y. Shen and X. Huang, “Attention-based convolutional neural network for semantic relation extraction,” in COLING, 2016, pp.2526–2536.词级别的注意力捕捉词的语义信息
<br>• Y. Lin, S. Shen, Z. Liu, H. Luan, and M. Sun, “Neural relation extraction with selective attention over instances,” in ACL, vol. 1,2016, pp. 2124–2133.对多个实例的选择性注意力，以减轻噪声实例的影响
<br>• G. Ji, K. Liu, S. He, and J. Zhao, “Distant supervision for relation extraction with sentence-level attention and entity descriptions,”in AAAI, 2017, pp. 3060–3066.APCNN引入了PCNN的实体描述和句子级注意
<br>• X. Han, P. Yu, Z. Liu, M. Sun, and P. Li, “Hierarchical relation extraction with coarse-to-fine grained attention,” in EMNLP, 2018,pp. 2236–2245.HATT提出分层选择性注意力，通过连接每个层级的注意力表示来捕获关系的层次
<br>• P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu, “Attention-based bidirectional long short-term memory networks for relation classification,” in ACL, vol. 2, 2016, pp. 207–212.Att-BLSTM没有使用基于CNN的句子编码器，而是使用BiLSTM提出了词级注意力</code></pre><h3 id="4-3-3-图卷积网络：GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码">4.3.3 图卷积网络：GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码<a class="post-anchor" href="#4-3-3-图卷积网络：GCNs用于对句子的依赖树进行编码，或学习KGEs来利用关系知识进行句子编码"></a></h3><pre><code>• Y. Zhang, P. Qi, and C. D. Manning, “Graph convolution over pruned dependency trees improves relation extraction,” in EMNLP, 2018, pp. 2205–2215.基于path-centric剪枝后的句子依赖树的上下文GCN模型
<br>• Z. Guo, Y. Zhang, and W. Lu, “Attention guided graph convolutional networks for relation extraction,” in ACL, 2019, pp. 241–251.AGGCN也在依赖树上应用GCN，但采用软加权方式的多头注意力方式进行边的选择
<br>• N. Zhang, S. Deng, Z. Sun, G. Wang, X. Chen, W. Zhang, and H. Chen, “Long-tail relation extraction via knowledge graph embeddings and graph convolution networks,” in NAACL, 2019,pp. 3016–3025.将GCN用于知识图谱中关系的embedding，从而实现基于语句的关系抽取。</code></pre><h3 id="4-3-4-对抗训练：在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声">4.3.4 对抗训练：在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声<a class="post-anchor" href="#4-3-4-对抗训练：在多标记多示例学习中，对抗训练被用于为基于CNN和RNN的关系抽取中的词embedding增加对抗噪声"></a></h3><pre><code>• Y. Wu, D. Bamman, and S. Russell, “Adversarial training for relation extraction,” in EMNLP, 2017, pp. 1778–1783.
<br>• P. Qin, X. Weiran, and W. Y. Wang, “DSGAN: Generative adversarial training for distant supervision relation extraction,” in ACL,vol. 1, 2018, pp. 496–505.DSGAN通过学习一个语句级true position的生成器和一个使生成器的true position概率最小化的识别器来去除远程监督关系抽取中的噪声</code></pre><h3 id="4-3-5-强化学习">4.3.5 强化学习<a class="post-anchor" href="#4-3-5-强化学习"></a></h3><p>近年来，通过用策略网络训练实例选择器的方式将强化学习集成到神经关系抽取中。基于强化学习的NRE的优点是关系抽取是 model-agnostic的。因此，它可以很容易地适应于任何神经结构，以有效地提取关系。</p>
<pre><code>• P. Qin, W. Xu, and W. Y. Wang, “Robust distant supervision relation extraction via deep reinforcement learning,” in ACL, vol. 1,2018, pp. 2137–2147 提出训练基于策略的句子关系分类器的RL代理将假阳性实例重新分布到负样本中，以减轻噪声数据的影响。以F1得分为评价指标，采用基于F1得分的性能变化作为策略网络的奖励
<br>• X. Zeng, S. He, K. Liu, and J. Zhao, “Large scaled relation  extraction with reinforcement learning,” in AAAI, 2018, pp. 5658–5665.和J. Feng, M. Huang, L. Zhao, Y. Yang, and X. Zhu, “Reinforcement  learning for relation classification from noisy data,” in AAAI, 2018,  pp. 5779–5786.提出了其他的奖励策略，
<br>• R. Takanobu, T. Zhang, J. Liu, and M. Huang, “A hierarchical  framework for relation extraction with reinforcement learning,”  in AAAI, vol. 33, 2019, pp. 7072–707 提出了一个高级关系检测和低级实体提取的层次策略学习框架</code></pre><h3 id="4-3-6-其他进展">4.3.6 其他进展<a class="post-anchor" href="#4-3-6-其他进展"></a></h3><pre><code>• Y. Huang and W. Y. Wang, “Deep residual learning for weakly-  supervised relation extraction,” in EMNLP, 2017, pp. 1803–1807.将深度残差学习应用到噪声关系提取中，发现9层cnn的性能得到了提高。
<br>• T. Liu, X. Zhang, W. Zhou, and W. Jia, “Neural relation extraction  via inner-sentence noise reduction and transfer learning,” in EMNLP, 2018, pp. 2195–2204.提出通过实体分类中的迁移学习来初始化神经模型
<br>• K. Lei, D. Chen, Y. Li, N. Du, M. Yang, W. Fan, and Y. Shen, “Co-  operative denoising for distantly supervised relation extraction,”  in COLING, 2018, pp. 426–436.通过双向知识蒸馏和自适应模拟，将文本语料库和知识图谱与外部逻辑规则集成在一起
<br>• H. Jiang, L. Cui, Z. Xu, D. Yang, J. Chen, C. Li, J. Liu, J. Liang, C. Wang, Y. Xiao, and W. Wang, “Relation extraction using  supervision from topic knowledge of relation labels,” in IJCAI, 2019, pp. 5024–5030.通过匹配句子和主题词，丰富句子表征学习
<br>• T. Gao, X. Han, Z. Liu, and M. Sun, “Hybrid attention-based  prototypical networks for noisy few-shot relation classification,”  in AAAI, vol. 33, 2019, pp. 6407–6414.提出了基于混合注意力的原型网络来计算原型关系embedding，并比较其和query的embedding之间的距离。</code></pre><p><a href="/imgs/2020071001.png" data-caption="关系抽取方法综述" data-fancybox="images"><img src="/imgs/2020071001.png" alt="关系抽取方法综述"></a></p>
</body></html>

  
    <div class="post-reward">
    <div id="reward-button">打赏</div>
      <div id="qr">
        <div class="wrap">
            
            <div class="bg-wrap">
              <a href="/images/zhifubao.jpg" target="_block" class="bg" style="background-image:url('/images/zhifubao.jpg')"></a>
              支付宝
            </div>
            
            
            <div class="bg-wrap">
                <a href="/images/weixin.jpg" target="_block" class="bg" style="background-image:url('/images/weixin.jpg')"></a>
              微信
            </div>
            
        </div>
      </div>
    </div>
  
  <div class="post-guide">
    <div class="item left">
        
    </div>
    <div class="item right">
        
          <a href="/2020/07/07/Lambda/">Lambda 架构</a>
        
    </div>
  </div>

  

  <div class="post-copyright">
    <div class="auth">
      本文作者：<a href="http://yimial.github.io">执杭</a>
    </div>
    <div class="link">
      永久链接：<a href="http://yimial.github.io/2020/07/09/2020%20KG%20survey%20KA/">http://yimial.github.io/2020/07/09/2020%20KG%20survey%20KA/</a>
    </div>
    <div class="declare">
      版权声明：本文首发于<a href="http://yimial.github.io">执杭</a>的博客，转载请注明出处！
    </div>
  </div>

  <div id="comment"></div>

  
  
</article>
        <footer>
          <div class="copyright">
            ©2020
            <a href="http://yimial.github.io">执杭</a> Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> |
            <a href="https://github.com/shixiaohu2206/hexo-theme-huhu" target="_blank" rel="noopener">hexo-theme-huhu</a>
          </div>
          
        </footer>
      </div>
    </div>
  </body>
  
</html>
<script type="text/javascript">
                  window.HUHU_CONFIG = JSON.parse("{\"share\":[\"weibo\",\"weixin\",\"qqkongjian\",\"QQ\",\"douban\",\"facebook\",\"twitter\",\"google\"],\"valine\":{\"API_ID\":\"\",\"API_KEY\":\"\"},\"service_worker\":{\"open\":false}}")
                </script> <script type="text/javascript">window.addEventListener('load', function() {
    
    window.loadJs = function(d, m, a) {
      var c = document.getElementsByTagName('head')[0] || document.head || document.documentElement
      var b = document.createElement('script')
      b.defer = true
      b.setAttribute('type', 'text/javascript')
      b.setAttribute('charset', 'UTF-8')
      b.setAttribute('async', 'true')
      b.setAttribute('src', d)
      m && b.setAttribute('data-main', '/scripts/app-built')
      if (typeof a === 'function') {
        if (window.attachEvent) {
          b.onreadystatechange = function() {
            var e = b.readyState
            if (e === 'loaded' || e === 'complete') {
              b.onreadystatechange = null
              a()
            }
          }
        } else {
          b.onload = a
        }
      }
      c.appendChild(b)
    }
    window.loadJs && window.loadJs('https://cdn.bootcss.com/require.js/2.3.6/require.min.js', true, function() {require.config({"paths":{"util":"util","share":"share","search":"search","pagemap":"pagemap.min","registerSW":"registerSW","valine":"cdn/Valine.min","av":["https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min"],"pjax":["https://cdn.bootcss.com/jquery.pjax/2.0.1/jquery.pjax.min"],"jquery":["https://cdn.bootcss.com/jquery/3.4.1/jquery.min"],"confirm":["https://cdn.bootcss.com/jquery-confirm/3.3.4/jquery-confirm.min"],"fancybox":["https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min"],"chart":["https://cdn.bootcss.com/Chart.js/2.8.0-rc.1/Chart.bundle.min"]},"map":{"*":{"css":"https://cdn.bootcss.com/require-css/0.1.10/css.min.js"}},"shim":{"fancybox":{"deps":["css!https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.css"]},"confirm":{"deps":["css!https://cdn.bootcss.com/jquery-confirm/3.3.4/jquery-confirm.min.css"]},"chart":{"deps":["css!https://cdn.bootcss.com/Chart.js/2.8.0-rc.1/Chart.min.css"]}},"waitSeconds":3})})
  })</script> <script type="text/javascript">
                  ;(function() {
                    var bp = document.createElement('script')
                    var curProtocol = window.location.protocol.split(':')[0]
                    if (curProtocol === 'https') {
                      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js'
                    } else {
                      bp.src = 'http://push.zhanzhang.baidu.com/push.js'
                    }
                    var s = document.getElementsByTagName('script')[0]
                    s.parentNode.insertBefore(bp, s)
                  })()
                </script> 
